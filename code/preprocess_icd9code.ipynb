{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MIMIC III Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('chartdate', 'date'), ('category', 'string'), ('description', 'string'), ('cgid', 'int'), ('iserror', 'int'), ('text', 'string')]\n",
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('seq_num', 'int'), ('icd9_code', 'string')]\n",
      "[('row_id', 'bigint'), ('subject_id', 'bigint'), ('hadm_id', 'bigint'), ('seq_num', 'bigint'), ('icd9_code', 'string')]\n",
      "[('hadm_id', 'int')]\n",
      "[('subject_id', 'int')]\n",
      "[('ROW_ID', 'int'), ('ICD9_CODE', 'string'), ('SHORT_TITLE', 'string'), ('LONG_TITLE', 'string')]\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()\n",
    "\n",
    "ne_struct = StructType([StructField(\"row_id\", IntegerType(), True),\n",
    "                      StructField(\"subject_id\", IntegerType(), True),\n",
    "                      StructField(\"hadm_id\", IntegerType(), True),\n",
    "                      StructField(\"chartdate\", DateType(), True),\n",
    "                      StructField(\"category\", StringType(), True),\n",
    "                      StructField(\"description\", StringType(), True),\n",
    "                      StructField(\"cgid\", IntegerType(), True),\n",
    "                      StructField(\"iserror\", IntegerType(), True),\n",
    "                      StructField(\"text\", StringType(), True)])\n",
    "df_ne = spark.read.csv(\"./data/NOTEEVENTS-2.csv\",\n",
    "# df_ne = spark.read.csv(\"./data/NOTEEVENTS-2sample.csv\",\n",
    "                       header=True,\n",
    "                       schema=ne_struct)\n",
    "df_ne.registerTempTable(\"noteevents\")\n",
    "df_ne.filter(df_ne.category==\"Discharge summary\") \\\n",
    "    .registerTempTable(\"noteevents2\")\n",
    "    \n",
    "# i want to cache noteevents, but it's too big\n",
    "\n",
    "# many icd to one hadm_id\n",
    "diag_struct = StructType([StructField(\"ROW_ID\", IntegerType(), True),\n",
    "                          StructField(\"SUBJECT_ID\", IntegerType(), True),\n",
    "                          StructField(\"HADM_ID\", IntegerType(), True),\n",
    "                          StructField(\"SEQ_NUM\", IntegerType(), True),\n",
    "                          StructField(\"ICD9_CODE\", StringType(), True)])\n",
    "df_diag_m = spark.read.csv(\"./data/DIAGNOSES_ICD.csv\",\n",
    "                           header=True,\n",
    "                           schema=diag_struct) \\\n",
    "            .selectExpr(\"ROW_ID as row_id\", \n",
    "                        \"SUBJECT_ID as subject_id\",\n",
    "                        \"HADM_ID as hadm_id\",\n",
    "                        \"SEQ_NUM as seq_num\",\n",
    "                        \"ICD9_CODE as icd9_code\")\n",
    "df_diag_m.registerTempTable(\"diagnoses_icd_m\")\n",
    "df_diag_m.cache()\n",
    "\n",
    "# one icd to one hadm_id (take the smallest seq number as primary)\n",
    "diag_o_rdd = df_diag_m.rdd.sortBy(lambda x: (x.hadm_id, x.subject_id, x.seq_num)) \\\n",
    "    .groupBy(lambda x: x.hadm_id) \\\n",
    "    .mapValues(list) \\\n",
    "    .reduceByKey(lambda x, y: x if x.seq_num < y.seq_num else y) \\\n",
    "    .map(lambda (hid, d): d[0])\n",
    "df_diag_o = spark.createDataFrame(diag_o_rdd)\n",
    "df_diag_o.registerTempTable(\"diagnoses_icd_o\")\n",
    "df_diag_o.cache()\n",
    "\n",
    "# get hadm_id list in noteevents\n",
    "df_hadm_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT hadm_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_hadm_id_list.registerTempTable(\"hadm_id_list\")\n",
    "df_hadm_id_list.cache()\n",
    "\n",
    "# get subject_id list in noteevents\n",
    "df_subject_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT subject_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_subject_id_list.registerTempTable(\"subject_id_list\")\n",
    "df_subject_id_list.cache()\n",
    "\n",
    "df_icd9desc = spark.read.csv(\"./data/D_ICD_DIAGNOSES.csv\",\n",
    "                       header=True, inferSchema=True)\n",
    "df_icd9desc.registerTempTable(\"diagnoses_icd_desc\")\n",
    "\n",
    "df_diag_o2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_o.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_o JOIN hadm_id_list\n",
    "ON diagnoses_icd_o.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_o2.registerTempTable(\"diagnoses_icd_o2\")\n",
    "df_diag_o2.cache()\n",
    "\n",
    "df_diag_m2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_m.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_m JOIN hadm_id_list\n",
    "ON diagnoses_icd_m.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_m2.registerTempTable(\"diagnoses_icd_m2\")\n",
    "df_diag_m2.cache()\n",
    "\n",
    "print df_ne.dtypes\n",
    "print df_diag_m.dtypes\n",
    "print df_diag_o.dtypes\n",
    "print df_hadm_id_list.dtypes\n",
    "print df_subject_id_list.dtypes\n",
    "print df_icd9desc.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preprocessing (all icd9 codes)\n",
    "\n",
    "Returns RDD[(hadm_id, list(icd9_codes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "icd9_score_hadm = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "icd9_score_subj = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "def get_id_to_topicd9(id_type, topX):\n",
    "    if id_type == \"hadm_id\":\n",
    "        icd9_score = icd9_score_hadm\n",
    "    else:\n",
    "        icd9_score = icd9_score_subj\n",
    "        \n",
    "    icd9_topX = set([i.icd9_code for i in icd9_score.takeOrdered(topX, key=lambda x: -x.score)])\n",
    "    \n",
    "    id_to_topicd9 = df_diag_m2.rdd \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.icd9_code)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: set(x) & icd9_topX) \\\n",
    "        .filter(lambda (x, y): y)\n",
    "        \n",
    "    return id_to_topicd9, list(icd9_topX)\n",
    "\n",
    "# for i in get_id_to_topicd9(\"hadm_id\", 10)[0].take(3):\n",
    "#     print i\n",
    "# for i in get_id_to_topicd9(\"subject_id\", 50)[0].take(3):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Obtain dataframe for the merged noteevents and ID-to-ICD9 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sparse2vec(mapper, data):\n",
    "    out = [0] * len(mapper)\n",
    "    if data != None:\n",
    "        for i in data:\n",
    "            out[mapper[i]] = 1\n",
    "    return out\n",
    "\n",
    "def get_id_to_texticd9(id_type, topX):\n",
    "    id_to_topicd9, topicd9 = get_id_to_topicd9(id_type, topX)\n",
    "    mapper = dict(zip(topicd9, range(topX)))\n",
    "    \n",
    "    ne_topX = df_ne.rdd \\\n",
    "        .filter(lambda x: x.category == \"Discharge summary\") \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.text)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: \" \".join(x)) \\\n",
    "        #.join(id_to_topicd9) \\ # involve only data related to top10\n",
    "        # involve all data, even those not related to top10\n",
    "        .leftOuterJoin(id_to_topicd9) \\\n",
    "        .map(lambda (id_, (text, icd9)): \\\n",
    "             [id_]+sparse2vec(mapper, icd9)+[text])\n",
    "#              list(Vectors.sparse(topX, dict.fromkeys(map(lambda x: mapper[x], icd9), 1))))\n",
    "        \n",
    "    return spark.createDataFrame(ne_topX, [\"id\"]+topicd9+[\"text\"]), mapper\n",
    "\n",
    "# get_id_to_texticd9(\"hadm_id\", 10)[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make list of unique ICD9CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ICD9CODES = spark.sql(\"\"\"\n",
    "SELECT DISTINCT icd9_code FROM diagnoses_icd_m2\n",
    "\"\"\").rdd.map(lambda x: x.icd9_code).collect()\n",
    "ICD9CODES = [str(i).lower() for i in ICD9CODES]\n",
    "\n",
    "pickle.dump(ICD9CODES, open( \"./data/ICD9CODES.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'4019': 0, u'2724': 1, u'25000': 2, u'4280': 3, u'41401': 4, u'42731': 7, u'5849': 8, u'53081': 5, u'51881': 6, u'5990': 9}\n",
      "52726\n"
     ]
    }
   ],
   "source": [
    "df_id2texticd9, topicd9_mapper = get_id_to_texticd9(\"hadm_id\", 10)\n",
    "df_id2texticd9.write.csv(\"./data/DATA_HADM_TOP10\", header=True)\n",
    "\n",
    "print topicd9_mapper\n",
    "print df_id2texticd9.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  4019  2724  25000  4280  41401  53081  51881  42731  5849  5990  \\\n",
      "0  117760     0     0      0     0      0      1      1      0     0     0   \n",
      "1  129030     1     1      0     0      0      1      0      0     0     0   \n",
      "2  172040     0     0      0     0      1      0      0      0     1     0   \n",
      "3  156170     0     0      1     1      0      0      0      1     1     0   \n",
      "4  199180     0     0      1     1      1      0      0      0     0     0   \n",
      "\n",
      "                                                text  \n",
      "0  \"Admission Date:  [**2118-12-14**]            ...  \n",
      "1  Admission Date:  [**2137-8-31**]              ...  \n",
      "2  Admission Date:  [**2174-1-6**]              D...  \n",
      "3  Admission Date:  [**2102-6-9**]              D...  \n",
      "4  Admission Date:  [**2164-7-2**]       Discharg...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/DATA_HADM_TOP10.csv\", escapechar='\\\\')\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test by counting rows (depreciated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|icd9_code|\n",
      "+---------+\n",
      "|     4019|\n",
      "|     4280|\n",
      "|    42731|\n",
      "|    41401|\n",
      "|     5849|\n",
      "|    25000|\n",
      "|     2724|\n",
      "|    51881|\n",
      "|     5990|\n",
      "|    53081|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n",
    "    \n",
    "# id_to_topicd9, topicd9 = get_id_to_topicd9(\"hadm_id\", 10)\n",
    "# print id_to_topicd9.count()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "# FROM diagnoses_icd_m2\n",
    "# WHERE icd9_code IN\n",
    "#     (SELECT icd9_code\n",
    "#     FROM diagnoses_icd_m2\n",
    "#     GROUP BY icd9_code\n",
    "#     ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "#     LIMIT 10)\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#sc.stop()\n",
    "print \"Done!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
