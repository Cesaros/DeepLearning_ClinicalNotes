{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC III Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('chartdate', 'date'), ('category', 'string'), ('description', 'string'), ('cgid', 'int'), ('iserror', 'int'), ('text', 'string')]\n",
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('seq_num', 'int'), ('icd9_code', 'string')]\n",
      "[('row_id', 'bigint'), ('subject_id', 'bigint'), ('hadm_id', 'bigint'), ('seq_num', 'bigint'), ('icd9_code', 'string')]\n",
      "[('hadm_id', 'int')]\n",
      "[('subject_id', 'int')]\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()\n",
    "\n",
    "ne_struct = StructType([StructField(\"row_id\", IntegerType(), True),\n",
    "                      StructField(\"subject_id\", IntegerType(), True),\n",
    "                      StructField(\"hadm_id\", IntegerType(), True),\n",
    "                      StructField(\"chartdate\", DateType(), True),\n",
    "                      StructField(\"category\", StringType(), True),\n",
    "                      StructField(\"description\", StringType(), True),\n",
    "                      StructField(\"cgid\", IntegerType(), True),\n",
    "                      StructField(\"iserror\", IntegerType(), True),\n",
    "                      StructField(\"text\", StringType(), True)])\n",
    "df_ne = spark.read.csv(\"./data/NOTEEVENTS-2.csv\",\n",
    "# df_ne = spark.read.csv(\"./data/NOTEEVENTS-2sample.csv\",\n",
    "                       header=True,\n",
    "                       schema=ne_struct)\n",
    "df_ne.registerTempTable(\"noteevents\")\n",
    "df_ne.filter(df_ne.category==\"Discharge summary\") \\\n",
    "    .registerTempTable(\"noteevents2\")\n",
    "    \n",
    "# i want to cache noteevents, but it's too big\n",
    "\n",
    "# many icd to one hadm_id\n",
    "diag_struct = StructType([StructField(\"ROW_ID\", IntegerType(), True),\n",
    "                          StructField(\"SUBJECT_ID\", IntegerType(), True),\n",
    "                          StructField(\"HADM_ID\", IntegerType(), True),\n",
    "                          StructField(\"SEQ_NUM\", IntegerType(), True),\n",
    "                          StructField(\"ICD9_CODE\", StringType(), True)])\n",
    "df_diag_m = spark.read.csv(\"./data/DIAGNOSES_ICD.csv\",\n",
    "                           header=True,\n",
    "                           schema=diag_struct) \\\n",
    "            .selectExpr(\"ROW_ID as row_id\", \n",
    "                        \"SUBJECT_ID as subject_id\",\n",
    "                        \"HADM_ID as hadm_id\",\n",
    "                        \"SEQ_NUM as seq_num\",\n",
    "                        \"ICD9_CODE as icd9_code\")\n",
    "df_diag_m.registerTempTable(\"diagnoses_icd_m\")\n",
    "df_diag_m.cache()\n",
    "\n",
    "# one icd to one hadm_id (take the smallest seq number as primary)\n",
    "diag_o_rdd = df_diag_m.rdd.sortBy(lambda x: (x.hadm_id, x.subject_id, x.seq_num)) \\\n",
    "    .groupBy(lambda x: x.hadm_id) \\\n",
    "    .mapValues(list) \\\n",
    "    .reduceByKey(lambda x, y: x if x.seq_num < y.seq_num else y) \\\n",
    "    .map(lambda (hid, d): d[0])\n",
    "df_diag_o = spark.createDataFrame(diag_o_rdd)\n",
    "df_diag_o.registerTempTable(\"diagnoses_icd_o\")\n",
    "df_diag_o.cache()\n",
    "\n",
    "# get hadm_id list in noteevents\n",
    "df_hadm_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT hadm_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_hadm_id_list.registerTempTable(\"hadm_id_list\")\n",
    "df_hadm_id_list.cache()\n",
    "\n",
    "# get subject_id list in noteevents\n",
    "df_subject_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT subject_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_subject_id_list.registerTempTable(\"subject_id_list\")\n",
    "df_subject_id_list.cache()\n",
    "\n",
    "print df_ne.dtypes\n",
    "print df_diag_m.dtypes\n",
    "print df_diag_o.dtypes\n",
    "print df_hadm_id_list.dtypes\n",
    "print df_subject_id_list.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: bigint, subject_id: bigint, hadm_id: bigint, seq_num: bigint, icd9_code: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diag_o2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_o.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_o JOIN hadm_id_list\n",
    "ON diagnoses_icd_o.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_o2.registerTempTable(\"diagnoses_icd_o2\")\n",
    "df_diag_o2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: int, subject_id: int, hadm_id: int, seq_num: int, icd9_code: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diag_m2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_m.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_m JOIN hadm_id_list\n",
    "ON diagnoses_icd_m.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_m2.registerTempTable(\"diagnoses_icd_m2\")\n",
    "df_diag_m2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|\n",
      "+--------+--------------------------+-----------------------+\n",
      "| 2083180|                     46146|                  58361|\n",
      "+--------+--------------------------+-----------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|\n",
      "+--------+--------------------------+-----------------------+\n",
      "|   59652|                     41127|                  52726|\n",
      "+--------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), COUNT(DISTINCT hadm_id)\n",
    "FROM noteevents\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), COUNT(DISTINCT hadm_id)\n",
    "FROM noteevents2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         category|\n",
      "+-----------------+\n",
      "|              ECG|\n",
      "|     Respiratory |\n",
      "|          Nursing|\n",
      "|          General|\n",
      "|          Consult|\n",
      "|             Echo|\n",
      "|        Nutrition|\n",
      "|       Physician |\n",
      "|         Pharmacy|\n",
      "|   Rehab Services|\n",
      "| Case Management |\n",
      "|        Radiology|\n",
      "|    Nursing/other|\n",
      "|Discharge summary|\n",
      "|      Social Work|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(category)\n",
    "FROM noteevents\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diagnoses_icd: many (icd_code) to one (hadm_id)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT ICD9_CODE)|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|  651047|                     46520|                  58976|                     6984|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT lower(ICD9_CODE))|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|  651047|                     46520|                  58976|                            6984|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT ICD9_CODE)\n",
    "FROM diagnoses_icd_m\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT LOWER(ICD9_CODE))\n",
    "FROM diagnoses_icd_m\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diagnoses_icd: one (icd_code) to one (hadm_id)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT ICD9_CODE)|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|   58976|                     46520|                  58976|                     2789|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT lower(ICD9_CODE))|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|   58976|                     46520|                  58976|                            2789|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT ICD9_CODE)\n",
    "FROM diagnoses_icd_o\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT LOWER(ICD9_CODE))\n",
    "FROM diagnoses_icd_o\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check if I really did get \"seq_num = 1\" for all diagnosis, the code below should return empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+\n",
      "|row_id|subject_id|hadm_id|seq_num|icd9_code|\n",
      "+------+----------+-------+-------+---------+\n",
      "+------+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check code\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM diagnoses_icd_o\n",
    "WHERE seq_num <> 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents and diagnoses_icd (one to one)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------+-------------------------+\n",
      "|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT icd9_code)|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "|                     41127|                  52726|                     2706|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT icd9_code)\n",
    "FROM diagnoses_icd_o2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 ICD 9 codes based on \"subject_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|icd9_code|sid_count|\n",
      "+---------+---------+\n",
      "|    41401|     3435|\n",
      "|     0389|     1837|\n",
      "|    41071|     1672|\n",
      "|    V3001|     1390|\n",
      "|     4241|     1128|\n",
      "|    51881|      986|\n",
      "|    V3000|      949|\n",
      "|      431|      948|\n",
      "|    V3101|      851|\n",
      "|      486|      654|\n",
      "|     5070|      581|\n",
      "|     4240|      552|\n",
      "|     4280|      513|\n",
      "|     5849|      500|\n",
      "|      430|      491|\n",
      "|    41011|      469|\n",
      "|    41041|      465|\n",
      "|     5789|      410|\n",
      "|     5770|      343|\n",
      "|    41519|      334|\n",
      "|     1983|      330|\n",
      "|    43411|      330|\n",
      "|    43491|      318|\n",
      "|    42731|      307|\n",
      "|    99859|      299|\n",
      "|    03842|      291|\n",
      "|    85221|      279|\n",
      "|    56212|      251|\n",
      "|    42823|      246|\n",
      "|    V3401|      235|\n",
      "|    99662|      229|\n",
      "|    42833|      229|\n",
      "|     4271|      221|\n",
      "|    51884|      220|\n",
      "|     4321|      214|\n",
      "|     5712|      212|\n",
      "|    99811|      209|\n",
      "|    49121|      208|\n",
      "|     4373|      202|\n",
      "|    03849|      201|\n",
      "|    85220|      197|\n",
      "|    03811|      192|\n",
      "|     4414|      184|\n",
      "|    25013|      183|\n",
      "|    43310|      177|\n",
      "|     0380|      174|\n",
      "|     1623|      171|\n",
      "|    53240|      166|\n",
      "|    44101|      165|\n",
      "|    29181|      164|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS sid_count\n",
    "FROM diagnoses_icd_o2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY sid_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 ICD 9 codes based on \"hadm_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|icd9_code|hadm_count|\n",
      "+---------+----------+\n",
      "|    41401|      3464|\n",
      "|     0389|      1976|\n",
      "|    41071|      1719|\n",
      "|    V3001|      1390|\n",
      "|     4241|      1136|\n",
      "|    51881|      1089|\n",
      "|      431|       966|\n",
      "|    V3000|       949|\n",
      "|    V3101|       851|\n",
      "|      486|       703|\n",
      "|     5070|       641|\n",
      "|     4240|       558|\n",
      "|     4280|       553|\n",
      "|     5849|       518|\n",
      "|      430|       495|\n",
      "|    41011|       472|\n",
      "|    41041|       467|\n",
      "|     5789|       432|\n",
      "|     5770|       365|\n",
      "|     1983|       355|\n",
      "|    41519|       337|\n",
      "|    43411|       331|\n",
      "|    43491|       318|\n",
      "|    42731|       317|\n",
      "|    99859|       308|\n",
      "|    03842|       304|\n",
      "|    85221|       289|\n",
      "|    99662|       285|\n",
      "|    25013|       284|\n",
      "|    42823|       284|\n",
      "|    56212|       267|\n",
      "|    42833|       267|\n",
      "|    49121|       264|\n",
      "|     4271|       255|\n",
      "|     5712|       247|\n",
      "|    51884|       245|\n",
      "|     4373|       242|\n",
      "|    V3401|       235|\n",
      "|     4321|       231|\n",
      "|    29181|       229|\n",
      "|    99811|       210|\n",
      "|    03849|       207|\n",
      "|    85220|       201|\n",
      "|    03811|       198|\n",
      "|      042|       187|\n",
      "|     4414|       184|\n",
      "|    43310|       180|\n",
      "|     0380|       176|\n",
      "|    44101|       174|\n",
      "|     1623|       173|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_o2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY hadm_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents and diagnoses_icd (many to one)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------+-------------------------+\n",
      "|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT icd9_code)|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "|                     41127|                  52726|                     6918|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT icd9_code)\n",
    "FROM diagnoses_icd_m2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top ICD 9 codes based on \"subject_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|icd9_code|sid_count|\n",
      "+---------+---------+\n",
      "|     4019|    17138|\n",
      "|    41401|    10579|\n",
      "|    42731|    10053|\n",
      "|     4280|     9669|\n",
      "|     5849|     7505|\n",
      "|     2724|     7324|\n",
      "|    25000|     7181|\n",
      "|    51881|     6493|\n",
      "|     5990|     5687|\n",
      "|     2720|     5199|\n",
      "|    53081|     5148|\n",
      "|     2859|     4895|\n",
      "|      486|     4329|\n",
      "|     2851|     4195|\n",
      "|     2762|     4021|\n",
      "|     2449|     3732|\n",
      "|      496|     3491|\n",
      "|    99592|     3449|\n",
      "|     5070|     3319|\n",
      "|     0389|     3304|\n",
      "|    V5861|     3126|\n",
      "|     3051|     2926|\n",
      "|    41071|     2861|\n",
      "|      311|     2859|\n",
      "|     5859|     2855|\n",
      "|    40390|     2784|\n",
      "|     2761|     2757|\n",
      "|     2875|     2751|\n",
      "|      412|     2723|\n",
      "|     4240|     2613|\n",
      "|     5119|     2528|\n",
      "|     V290|     2519|\n",
      "|    V1582|     2486|\n",
      "|    78552|     2335|\n",
      "|     4241|     2285|\n",
      "|     9971|     2278|\n",
      "|    42789|     2259|\n",
      "|    V4581|     2256|\n",
      "|    V4582|     2202|\n",
      "|     7742|     2174|\n",
      "|     5845|     2122|\n",
      "|     V053|     2116|\n",
      "|     5180|     2055|\n",
      "|     2760|     2054|\n",
      "|    45829|     2035|\n",
      "|    V5867|     1987|\n",
      "|     2767|     1877|\n",
      "|     4589|     1862|\n",
      "|     4168|     1780|\n",
      "|     5185|     1753|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS sid_count\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY sid_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top ICD 9 codes based on \"hadm_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|icd9_code|hadm_count|\n",
      "+---------+----------+\n",
      "|     4019|     20046|\n",
      "|     4280|     12842|\n",
      "|    42731|     12589|\n",
      "|    41401|     12178|\n",
      "|     5849|      8906|\n",
      "|    25000|      8783|\n",
      "|     2724|      8503|\n",
      "|    51881|      7249|\n",
      "|     5990|      6442|\n",
      "|    53081|      6154|\n",
      "|     2720|      5766|\n",
      "|     2859|      5295|\n",
      "|     2449|      4785|\n",
      "|      486|      4732|\n",
      "|     2851|      4499|\n",
      "|     2762|      4358|\n",
      "|      496|      4296|\n",
      "|    99592|      3792|\n",
      "|    V5861|      3697|\n",
      "|     5070|      3592|\n",
      "|     0389|      3580|\n",
      "|     5859|      3367|\n",
      "|    40390|      3350|\n",
      "|      311|      3347|\n",
      "|     3051|      3272|\n",
      "|      412|      3203|\n",
      "|     2875|      3002|\n",
      "|    41071|      3001|\n",
      "|     2761|      2985|\n",
      "|    V4581|      2943|\n",
      "|     4240|      2876|\n",
      "|    V1582|      2741|\n",
      "|     5119|      2693|\n",
      "|    V4582|      2651|\n",
      "|    40391|      2566|\n",
      "|     V290|      2529|\n",
      "|     4241|      2517|\n",
      "|    78552|      2501|\n",
      "|    V5867|      2497|\n",
      "|    42789|      2396|\n",
      "|    32723|      2328|\n",
      "|     9971|      2313|\n",
      "|     5845|      2223|\n",
      "|     2760|      2221|\n",
      "|     7742|      2183|\n",
      "|     5180|      2137|\n",
      "|     V053|      2119|\n",
      "|     4168|      2118|\n",
      "|    49390|      2113|\n",
      "|     2767|      2111|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY hadm_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (all icd9 codes)\n",
    "\n",
    "Returns RDD[(hadm_id, list(icd9_codes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "icd9_score_hadm = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "icd9_score_subj = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "def get_id_to_topicd9(id_type, topX):\n",
    "    if id_type == \"hadm_id\":\n",
    "        icd9_score = icd9_score_hadm\n",
    "    else:\n",
    "        icd9_score = icd9_score_subj\n",
    "        \n",
    "    icd9_topX = set([i.icd9_code for i in icd9_score.takeOrdered(topX, key=lambda x: -x.score)])\n",
    "    \n",
    "    id_to_topicd9 = df_diag_m2.rdd \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.icd9_code)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: set(x) & icd9_topX) \\\n",
    "        .filter(lambda (x, y): y)\n",
    "        \n",
    "    return id_to_topicd9, list(icd9_topX)\n",
    "\n",
    "# for i in get_id_to_topicd9(\"hadm_id\", 10)[0].take(3):\n",
    "#     print i\n",
    "# for i in get_id_to_topicd9(\"subject_id\", 50)[0].take(3):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain dataframe for the merged noteevents and ID-to-ICD9 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparse2vec(mapper, data):\n",
    "    out = [0] * len(mapper)\n",
    "    for i in data:\n",
    "        out[mapper[i]] = 1\n",
    "    return out\n",
    "\n",
    "def get_id_to_texticd9(id_type, topX):\n",
    "    id_to_topicd9, topicd9 = get_id_to_topicd9(id_type, topX)\n",
    "    mapper = dict(zip(topicd9, range(topX)))\n",
    "    \n",
    "    ne_topX = df_ne.rdd \\\n",
    "        .filter(lambda x: x.category == \"Discharge summary\") \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.text)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: \" \".join(x)) \\\n",
    "        .join(id_to_topicd9) \\\n",
    "        .map(lambda (id_, (text, icd9)): \\\n",
    "             [id_, text]+sparse2vec(mapper, icd9))\n",
    "#              list(Vectors.sparse(topX, dict.fromkeys(map(lambda x: mapper[x], icd9), 1))))\n",
    "        \n",
    "    return spark.createDataFrame(ne_topX, [\"id\", \"text\"]+topicd9), mapper\n",
    "\n",
    "# get_id_to_texticd9(\"hadm_id\", 10)[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Input df must be RDD[(label, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer, StopWordsRemover\n",
    "\n",
    "def create_TFIDF(sentenceData, inputCol=\"text\", outputCol=\"features\", minDocFreq=3, numFeatures=20):\n",
    "    tokenizer = RegexTokenizer(pattern=\"[.:\\s]+\", inputCol=inputCol, outputCol=\"z_words\")\n",
    "    wordsData = tokenizer.transform(sentenceData)\n",
    "    \n",
    "    remover = StopWordsRemover(inputCol=\"z_words\", outputCol=\"z_filtered\")\n",
    "    wordsDataFiltered = remover.transform(wordsData)\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"z_filtered\", outputCol=\"z_rawFeatures\", numFeatures=numFeatures)\n",
    "    featurizedData = hashingTF.transform(wordsDataFiltered)\n",
    "    # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "    idf = IDF(inputCol=\"z_rawFeatures\", outputCol=outputCol, minDocFreq=minDocFreq)\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    return rescaledData.drop(\"z_words\", \"z_filtered\", \"z_rawFeatures\", inputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'4019': 0, u'2724': 1, u'25000': 2, u'4280': 3, u'41401': 4, u'42731': 7, u'5849': 8, u'53081': 5, u'51881': 6, u'5990': 9}\n",
      "[('id', 'bigint'), ('4019', 'bigint'), ('2724', 'bigint'), ('25000', 'bigint'), ('4280', 'bigint'), ('41401', 'bigint'), ('53081', 'bigint'), ('51881', 'bigint'), ('42731', 'bigint'), ('5849', 'bigint'), ('5990', 'bigint'), ('features', 'vector')]\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|117760|   0|   0|    0|   0|    0|    1|    1|    0|   0|   0|(40000,[69,372,69...|\n",
      "|129030|   1|   1|    0|   0|    0|    1|    0|    0|   0|   0|(40000,[13,32,83,...|\n",
      "|172040|   0|   0|    0|   0|    1|    0|    0|    0|   1|   0|(40000,[10,69,152...|\n",
      "|156170|   0|   0|    1|   1|    0|    0|    0|    1|   1|   0|(40000,[3,78,130,...|\n",
      "|199180|   0|   0|    1|   1|    1|    0|    0|    0|   0|   0|(40000,[48,62,80,...|\n",
      "|167440|   0|   0|    1|   0|    1|    0|    1|    0|   0|   0|(40000,[207,264,2...|\n",
      "|178710|   0|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[574,794,1...|\n",
      "|162840|   0|   1|    0|   1|    0|    0|    0|    1|   0|   0|(40000,[32,130,15...|\n",
      "|189980|   1|   0|    1|   1|    0|    0|    0|    0|   0|   0|(40000,[75,207,33...|\n",
      "|142370|   0|   0|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[22,30,207...|\n",
      "|153640|   0|   0|    0|   0|    1|    0|    1|    0|   0|   0|(40000,[62,63,85,...|\n",
      "|180780|   0|   0|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[207,648,7...|\n",
      "|149040|   1|   0|    0|   0|    1|    0|    1|    0|   0|   0|(40000,[574,794,1...|\n",
      "|101430|   1|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[33,63,71,...|\n",
      "|132020|   1|   1|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[130,187,2...|\n",
      "|155710|   1|   0|    0|   1|    0|    1|    1|    0|   1|   0|(40000,[63,92,115...|\n",
      "|151110|   0|   0|    0|   0|    0|    0|    0|    1|   1|   0|(40000,[63,73,169...|\n",
      "|140300|   0|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[37,83,106...|\n",
      "|103500|   1|   0|    1|   0|    1|    0|    0|    1|   0|   0|(40000,[12,35,130...|\n",
      "|130640|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[207,312,5...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_id2texticd9, topicd9_mapper = get_id_to_texticd9(\"hadm_id\", 10)\n",
    "df_id2featurelabel = create_TFIDF(df_id2texticd9, numFeatures=40000)\n",
    "\n",
    "print topicd9_mapper\n",
    "print df_id2featurelabel.dtypes\n",
    "df_id2featurelabel.show()\n",
    "\n",
    "output_csv(df_id2featurelabel, \"./data/DATA_TFIDF_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Test] Load csv file\n",
    "count should be the same with the sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40562\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|185344|   0|   0|    0|   0|    1|    0|    0|    1|   0|   1|(40000,[20,32,69,...|\n",
      "|169474|   1|   0|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[63,80,207...|\n",
      "|180054|   1|   0|    0|   0|    0|    0|    0|    0|   1|   1|(40000,[32,115,13...|\n",
      "|137734|   0|   1|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[48,148,20...|\n",
      "|121864|   1|   0|    0|   1|    0|    0|    0|    1|   0|   0|(40000,[273,379,8...|\n",
      "|115884|   1|   0|    1|   0|    0|    0|    0|    0|   0|   0|(40000,[100,361,5...|\n",
      "|105994|   1|   0|    0|   1|    0|    0|    0|    0|   0|   0|(40000,[20,32,207...|\n",
      "|110594|   0|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[78,107,14...|\n",
      "|176144|   0|   1|    1|   1|    1|    0|    0|    0|   0|   0|(40000,[62,130,20...|\n",
      "|134744|   0|   0|    1|   1|    0|    0|    0|    0|   0|   0|(40000,[2,207,307...|\n",
      "|101394|   1|   0|    1|   0|    0|    0|    0|    1|   0|   0|(40000,[148,207,7...|\n",
      "|171544|   0|   0|    0|   0|    1|    1|    0|    0|   0|   0|(40000,[20,32,48,...|\n",
      "|198684|   0|   0|    1|   1|    0|    0|    0|    0|   1|   0|(40000,[130,193,2...|\n",
      "|118874|   1|   0|    1|   0|    0|    0|    0|    0|   0|   0|(40000,[63,130,18...|\n",
      "|123934|   0|   1|    0|   0|    1|    0|    0|    1|   0|   0|(40000,[104,207,5...|\n",
      "|166944|   0|   0|    0|   1|    0|    0|    0|    0|   0|   0|(40000,[69,130,14...|\n",
      "|151074|   0|   0|    0|   1|    1|    0|    0|    1|   0|   0|(40000,[63,192,37...|\n",
      "|194084|   1|   0|    0|   1|    0|    0|    0|    0|   0|   0|(40000,[273,794,1...|\n",
      "|196614|   0|   0|    0|   1|    0|    0|    0|    1|   0|   0|(40000,[20,63,108...|\n",
      "|178214|   1|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[1,20,32,2...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdf = read_csv(\"./data/DATA_TFIDF_HADM_TOP10\")\n",
    "print testdf.count()\n",
    "testdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|icd9_code|\n",
      "+---------+\n",
      "|     4019|\n",
      "|     4280|\n",
      "|    42731|\n",
      "|    41401|\n",
      "|     5849|\n",
      "|    25000|\n",
      "|     2724|\n",
      "|    51881|\n",
      "|     5990|\n",
      "|    53081|\n",
      "+---------+\n",
      "\n",
      "40562\n",
      "+----------+\n",
      "|hadm_count|\n",
      "+----------+\n",
      "|     40562|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n",
    "    \n",
    "id_to_topicd9, topicd9 = get_id_to_topicd9(\"hadm_id\", 10)\n",
    "print id_to_topicd9.count()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_m2\n",
    "WHERE icd9_code IN\n",
    "    (SELECT icd9_code\n",
    "    FROM diagnoses_icd_m2\n",
    "    GROUP BY icd9_code\n",
    "    ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "    LIMIT 10)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 ICD 9 codes category (cleaned) -- to follow\n",
    "### Top 50 ICD 9 codes category (cleaned) -- to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
