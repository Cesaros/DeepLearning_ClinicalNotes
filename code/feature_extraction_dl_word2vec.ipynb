{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Deep Learning (WORD2VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "ICD9CODES = pickle.load(open(\"./data/ICD9CODES.p\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC_DL_V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS_WORD2VEC = stopwords.words('english') + ICD9CODES\n",
    "\n",
    "def preprocessor_word2vec(text):\n",
    "    text = re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', text)\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \n",
    "    text = re.sub(\" \\d+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_WORD2VEC_DL_V0(df, max_sequence_len=600, inputCol='text'):\n",
    "    texts = df[inputCol].apply(preprocessor_word2vec)\n",
    "    #texts = df['text']  # list of text samples\n",
    "\n",
    "    toke = Tokenizer()\n",
    "    toke.fit_on_texts(texts)\n",
    "    sequence = toke.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = toke.word_index\n",
    "    reverse_word_index = dict(zip(word_index.values(), word_index.keys())) # dict e.g. {1:'the', 2:'a' ...}\n",
    "    #index_list = word_index.values()\n",
    "\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequence, maxlen=max_sequence_len)\n",
    "    \n",
    "    return data, word_index, reverse_word_index\n",
    "\n",
    "def create_EmbeddingMatrix_V0(word_index, word2vec_model_path, remove_stopwords=True):\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(word2vec_model_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # Delete stopwords and ICD9 codes from pre-trained dictionary , \n",
    "        # so they will be zeros when we create embedding_matrix\n",
    "        keys_updated = [word for word in embeddings_index.keys() if word not in STOPWORDS_WORD2VEC]\n",
    "        index2word_set=set(keys_updated)\n",
    "    else:\n",
    "        index2word_set=set(embeddings_index.keys())\n",
    "    \n",
    "    EMBEDDING_DIM = embeddings_index.values()[0].size  # dimensions of the word2vec model\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if word in index2word_set: \n",
    "            #embedding_vector = embeddings_index.get(word)\n",
    "        #if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embeddings_index.get(word)\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, cPickle\n",
    "\n",
    "def separate(seed, N):    \n",
    "    idx=list(range(N))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idx)\n",
    "    idx_train= idx[0:int(N*0.50)]\n",
    "    idx_val= idx[int(N*0.50):int(N*0.75)]\n",
    "    idx_test= idx[int(N*0.75):N]\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def separate_2(df, hadmid_pickle):\n",
    "    f = open(hadmid_pickle, 'rb')\n",
    "    hadmid_train = cPickle.load(f)\n",
    "    hadmid_val = cPickle.load(f)\n",
    "    hadmid_test = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    df2['_idx'] = df2.index\n",
    "    df2.set_index('id', inplace=True)\n",
    "    \n",
    "    idx_train = df2.loc[hadmid_train]['_idx'].tolist()\n",
    "    idx_val = df2.loc[hadmid_val]['_idx'].tolist()\n",
    "    idx_test = df2.loc[hadmid_test]['_idx'].tolist()\n",
    "    \n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def batch_output_pickle(df, data, reversemap, fname, labels, hadmid_pickle='./data/TRAIN-VAL-TEST-HADMID.p'):\n",
    "    idx_tuple = separate_2(df, hadmid_pickle)\n",
    "    \n",
    "    f = open(fname, 'wb')\n",
    "    cPickle.dump(reversemap, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    for i in idx_tuple:\n",
    "        cPickle.dump(data[i], f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    for i in idx_tuple:\n",
    "        cPickle.dump(df.loc[i][labels].values, f, protocol=cPickle.HIGHEST_PROTOCOL)        \n",
    "    f.close()\n",
    "    \n",
    "def output_pickle(obj, fname):\n",
    "    f = open(fname, 'wb')\n",
    "    cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  4019  2724  25000  4280  41401  53081  51881  42731  5849  5990  \\\n",
      "0  117760     0     0      0     0      0      1      1      0     0     0   \n",
      "1  129030     1     1      0     0      0      1      0      0     0     0   \n",
      "2  172040     0     0      0     0      1      0      0      0     1     0   \n",
      "3  156170     0     0      1     1      0      0      0      1     1     0   \n",
      "4  199180     0     0      1     1      1      0      0      0     0     0   \n",
      "\n",
      "                                                text  \n",
      "0  \"Admission Date:  [**2118-12-14**]            ...  \n",
      "1  Admission Date:  [**2137-8-31**]              ...  \n",
      "2  Admission Date:  [**2174-1-6**]              D...  \n",
      "3  Admission Date:  [**2102-6-9**]              D...  \n",
      "4  Admission Date:  [**2164-7-2**]       Discharg...  \n",
      "Found 117458 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/DATA_HADM_TOP10.csv\", escapechar='\\\\')\n",
    "print df.head()\n",
    "\n",
    "data, word_index, reverse_word_index = create_WORD2VEC_DL_V0(df, max_sequence_len=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [`4019`, `2724`,`25000`,`4280`,`41401`,`53081`,`51881`,`42731`,`5849`,`5990`]\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV0_HADM_TOP10.p\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33837 word vectors.\n",
      "Found 29939 word vectors.\n"
     ]
    }
   ],
   "source": [
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIX_WORD2VEC.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_100dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIX_WORD2VEC_v2_100dim.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ./data/TRAIN-VAL-TEST-HADMID.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "train = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_train.csv\", escapechar='\\\\')\n",
    "val = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_val.csv\", escapechar='\\\\')\n",
    "test = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_test.csv\", escapechar='\\\\')\n",
    "\n",
    "train2 = train['id'].tolist()\n",
    "val2 = val['id'].tolist()\n",
    "test2 = test['id'].tolist()\n",
    "\n",
    "f = open('./data/TRAIN-VAL-TEST-HADMID.p', 'wb')\n",
    "for obj in [train2, val2, test2]:\n",
    "    cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
