{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Deep Learning (WORD2VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, cPickle\n",
    "\n",
    "ICD9CODES = pickle.load(open(\"./data/ICD9CODES.p\", \"r\"))\n",
    "ICD9CODES_TOP10 = pickle.load(open(\"./data/ICD9CODES_TOP10.p\", \"r\"))\n",
    "ICD9CODES_TOP50 = pickle.load(open(\"./data/ICD9CODES_TOP50.p\", \"r\"))\n",
    "ICD9CAT_TOP10 = pickle.load(open(\"./data/ICD9CAT_TOP10.p\", \"r\"))\n",
    "ICD9CAT_TOP50 = pickle.load(open(\"./data/ICD9CAT_TOP50.p\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC_DL_V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS_WORD2VEC = stopwords.words('english') + ICD9CODES\n",
    "\n",
    "def preprocessor_word2vec(text):\n",
    "    text = re.sub('\\[\\*\\*[^\\]]*\\*\\*\\]', '', text)\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \n",
    "    text = re.sub(\" \\d+\", \" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_WORD2VEC_DL_V0(df, max_sequence_len=600, inputCol='text'):\n",
    "    texts = df[inputCol].apply(preprocessor_word2vec)\n",
    "    #texts = df['text']  # list of text samples\n",
    "\n",
    "    toke = Tokenizer()\n",
    "    toke.fit_on_texts(texts)\n",
    "    sequence = toke.texts_to_sequences(texts)\n",
    "\n",
    "    ave_seq = [len(i) for i in sequence]\n",
    "    print 1.0* sum(ave_seq) / len(ave_seq)\n",
    "    \n",
    "    word_index = toke.word_index\n",
    "    reverse_word_index = dict(zip(word_index.values(), word_index.keys())) # dict e.g. {1:'the', 2:'a' ...}\n",
    "    #index_list = word_index.values()\n",
    "\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequence, maxlen=max_sequence_len)\n",
    "    \n",
    "    return data, word_index, reverse_word_index\n",
    "\n",
    "def create_EmbeddingMatrix_V0(word_index, word2vec_model_path, remove_stopwords=True):\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(word2vec_model_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # Delete stopwords and ICD9 codes from pre-trained dictionary , \n",
    "        # so they will be zeros when we create embedding_matrix\n",
    "        keys_updated = [word for word in embeddings_index.keys() if word not in STOPWORDS_WORD2VEC]\n",
    "        index2word_set=set(keys_updated)\n",
    "    else:\n",
    "        index2word_set=set(embeddings_index.keys())\n",
    "    \n",
    "    EMBEDDING_DIM = embeddings_index.values()[0].size  # dimensions of the word2vec model\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if word in index2word_set: \n",
    "            #embedding_vector = embeddings_index.get(word)\n",
    "        #if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embeddings_index.get(word)\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, cPickle\n",
    "\n",
    "def separate(seed, N):    \n",
    "    idx=list(range(N))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(idx)\n",
    "    idx_train= idx[0:int(N*0.50)]\n",
    "    idx_val= idx[int(N*0.50):int(N*0.75)]\n",
    "    idx_test= idx[int(N*0.75):N]\n",
    "\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def separate_2(df, hadmid_pickle):\n",
    "    f = open(hadmid_pickle, 'rb')\n",
    "    hadmid_train = cPickle.load(f)\n",
    "    hadmid_val = cPickle.load(f)\n",
    "    hadmid_test = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    df2['_idx'] = df2.index\n",
    "    df2.set_index('id', inplace=True)\n",
    "    \n",
    "    idx_train = df2.loc[hadmid_train]['_idx'].tolist()\n",
    "    idx_val = df2.loc[hadmid_val]['_idx'].tolist()\n",
    "    idx_test = df2.loc[hadmid_test]['_idx'].tolist()\n",
    "    \n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def batch_output_pickle(df, data, reversemap, fname, labels, hadmid_pickle='./data/TRAIN-VAL-TEST-HADMID.p'):\n",
    "    idx_tuple = separate_2(df, hadmid_pickle)\n",
    "    \n",
    "    f = open(fname, 'wb')\n",
    "    cPickle.dump(reversemap, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    for i in idx_tuple:\n",
    "        cPickle.dump(data[i], f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    for i in idx_tuple:\n",
    "        cPickle.dump(df.loc[i][labels].values, f, protocol=cPickle.HIGHEST_PROTOCOL)        \n",
    "    f.close()\n",
    "    \n",
    "def output_pickle(obj, fname):\n",
    "    f = open(fname, 'wb')\n",
    "    cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  4019  4280  42731  41401  5849  25000  2724  51881  5990  \\\n",
      "0  117760     0     0      0      0     0      0     0      1     0   \n",
      "1  129030     1     0      0      0     0      0     1      0     0   \n",
      "2  172040     0     0      0      1     1      0     0      0     0   \n",
      "3  156170     0     1      1      0     1      1     0      0     0   \n",
      "4  199180     0     1      0      1     0      1     0      0     0   \n",
      "\n",
      "                         ...                          c511  c412  c707  c348  \\\n",
      "0                        ...                             0     0     0     1   \n",
      "1                        ...                             0     0     0     0   \n",
      "2                        ...                             0     0     0     0   \n",
      "3                        ...                             0     0     0     0   \n",
      "4                        ...                             0     0     0     0   \n",
      "\n",
      "   c765  cE88  c571  c300  c733  \\\n",
      "0     0     0     0     0     0   \n",
      "1     0     0     0     0     0   \n",
      "2     0     1     0     0     0   \n",
      "3     0     0     0     0     0   \n",
      "4     0     0     0     0     0   \n",
      "\n",
      "                                                text  \n",
      "0  \"Admission Date:  [**2118-12-14**]            ...  \n",
      "1  Admission Date:  [**2137-8-31**]              ...  \n",
      "2  Admission Date:  [**2174-1-6**]              D...  \n",
      "3  Admission Date:  [**2102-6-9**]              D...  \n",
      "4  Admission Date:  [**2164-7-2**]       Discharg...  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "1057.45596101\n",
      "Found 117458 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/DATA_HADM.csv\", escapechar='\\\\')\n",
    "print df.head()\n",
    "\n",
    "data, word_index, reverse_word_index = create_WORD2VEC_DL_V0(df.copy(), max_sequence_len=2000)\n",
    "output_pickle(word_index, \"./data/DATA_WORDSEQV0_WORDINDEX.p\")\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV0_HADM_TOP10.p\", ICD9CODES_TOP10)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV0_HADM_TOP50.p\", ICD9CODES_TOP50)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV0_HADM_TOP10CAT.p\", ICD9CAT_TOP10)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV0_HADM_TOP50CAT.p\", ICD9CAT_TOP50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33837 word vectors.\n",
      "Found 29939 word vectors.\n",
      "Found 29939 word vectors.\n",
      "Found 29939 word vectors.\n"
     ]
    }
   ],
   "source": [
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_WORD2VEC.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_100dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_WORD2VEC_v2_100dim.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_300dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_WORD2VEC_v2_300dim.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_600dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_WORD2VEC_v2_600dim.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  4019  4280  42731  41401  5849  25000  2724  51881  5990  \\\n",
      "0  117760     0     0      0      0     0      0     0      1     0   \n",
      "1  129030     1     0      0      0     0      0     1      0     0   \n",
      "2  172040     0     0      0      1     1      0     0      0     0   \n",
      "3  156170     0     1      1      0     1      1     0      0     0   \n",
      "4  199180     0     1      0      1     0      1     0      0     0   \n",
      "\n",
      "                         ...                          c511  c412  c707  c348  \\\n",
      "0                        ...                             0     0     0     1   \n",
      "1                        ...                             0     0     0     0   \n",
      "2                        ...                             0     0     0     0   \n",
      "3                        ...                             0     0     0     0   \n",
      "4                        ...                             0     0     0     0   \n",
      "\n",
      "   c765  cE88  c571  c300  c733  \\\n",
      "0     0     0     0     0     0   \n",
      "1     0     0     0     0     0   \n",
      "2     0     1     0     0     0   \n",
      "3     0     0     0     0     0   \n",
      "4     0     0     0     0     0   \n",
      "\n",
      "                                                text  \n",
      "0  admission date discharge date date birth sex s...  \n",
      "1  admission date discharge date date birth sex s...  \n",
      "2  admission date discharge date service medicine...  \n",
      "3  admission date discharge date service medicine...  \n",
      "4  admission date discharge date service ccu hist...  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "715.766851269\n",
      "Found 117328 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/DATA_HADM_CLEANED.csv\", escapechar='\\\\')\n",
    "print df.head()\n",
    "\n",
    "data, word_index, reverse_word_index = create_WORD2VEC_DL_V0(df.copy(), max_sequence_len=1500)\n",
    "output_pickle(word_index, \"./data/DATA_WORDSEQV1_WORDINDEX.p\")\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV1_HADM_TOP10.p\", ICD9CODES_TOP10)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV1_HADM_TOP50.p\", ICD9CODES_TOP50)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV1_HADM_TOP10CAT.p\", ICD9CAT_TOP10)\n",
    "batch_output_pickle(df, data, reverse_word_index, \"./data/DATA_WORDSEQV1_HADM_TOP50CAT.p\", ICD9CAT_TOP50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33837 word vectors.\n",
      "Found 29939 word vectors.\n",
      "Found 29939 word vectors.\n",
      "Found 29939 word vectors.\n"
     ]
    }
   ],
   "source": [
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_WORD2VEC.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_100dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_WORD2VEC_v2_100dim.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_300dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_WORD2VEC_v2_300dim.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/model_word2vec_v2_600dim.txt\", remove_stopwords=True)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_WORD2VEC_v2_600dim.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2231687 word vectors.\n",
      "Found 2231687 word vectors.\n",
      "Found 2231687 word vectors.\n",
      "Found 2231687 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word_index = cPickle.load(open(\"./data/DATA_WORDSEQV0_WORDINDEX.p\", \"rb\"))\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/bio_nlp_vec/PubMed-shuffle-win-30.txt\", remove_stopwords=False)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_BIONLPVEC_Pubmedshufflewin30F.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/bio_nlp_vec/PubMed-shuffle-win-2.txt\", remove_stopwords=False)\n",
    "output_pickle(em, \"./data/EMBMATRIXV0_BIONLPVEC_Pubmedshufflewin02F.p\")\n",
    "\n",
    "word_index = cPickle.load(open(\"./data/DATA_WORDSEQV1_WORDINDEX.p\", \"rb\"))\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/bio_nlp_vec/PubMed-shuffle-win-30.txt\", remove_stopwords=False)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_BIONLPVEC_Pubmedshufflewin30F.p\")\n",
    "em = create_EmbeddingMatrix_V0(word_index, \"./data/bio_nlp_vec/PubMed-shuffle-win-2.txt\", remove_stopwords=False)\n",
    "output_pickle(em, \"./data/EMBMATRIXV1_BIONLPVEC_Pubmedshufflewin02F.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  1057.45596101\n",
      "median:  962.0\n",
      "max:  7944\n",
      "min:  3\n",
      "90th percentile:  2016.0\n",
      "95th percentile:  2393.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/DATA_HADM.csv\", escapechar='\\\\')\n",
    "\n",
    "texts = df['text'].apply(preprocessor_word2vec)\n",
    "#texts = df['text']  # list of text samples\n",
    "\n",
    "toke = Tokenizer()\n",
    "toke.fit_on_texts(texts)\n",
    "sequence = toke.texts_to_sequences(texts)\n",
    "\n",
    "seq_len = [len(i) for i in sequence]\n",
    "print \"mean: \", np.mean(seq_len)\n",
    "print \"median: \", np.median(seq_len)\n",
    "print \"max: \", np.max(seq_len)\n",
    "print \"min: \", np.min(seq_len)\n",
    "print \"90th percentile: \", np.percentile(seq_len, 90)\n",
    "print \"95th percentile: \", np.percentile(seq_len, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  715.766851269\n",
      "median:  648.0\n",
      "max:  5338\n",
      "min:  2\n",
      "90th percentile:  1376.0\n",
      "95th percentile:  1622.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/DATA_HADM_CLEANED.csv\", escapechar='\\\\')\n",
    "\n",
    "texts = df['text'].apply(preprocessor_word2vec)\n",
    "#texts = df['text']  # list of text samples\n",
    "\n",
    "toke = Tokenizer()\n",
    "toke.fit_on_texts(texts)\n",
    "sequence = toke.texts_to_sequences(texts)\n",
    "\n",
    "seq_len = [len(i) for i in sequence]\n",
    "print \"mean: \", np.mean(seq_len)\n",
    "print \"median: \", np.median(seq_len)\n",
    "print \"max: \", np.max(seq_len)\n",
    "print \"min: \", np.min(seq_len)\n",
    "print \"90th percentile: \", np.percentile(seq_len, 90)\n",
    "print \"95th percentile: \", np.percentile(seq_len, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ./data/TRAIN-VAL-TEST-HADMID.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "train = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_train.csv\", escapechar='\\\\')\n",
    "val = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_val.csv\", escapechar='\\\\')\n",
    "test = pd.read_csv(\"./data/DATA_TFIDFV1_HADM_TOP10_test.csv\", escapechar='\\\\')\n",
    "\n",
    "train2 = train['id'].tolist()\n",
    "val2 = val['id'].tolist()\n",
    "test2 = test['id'].tolist()\n",
    "\n",
    "f = open('./data/TRAIN-VAL-TEST-HADMID.p', 'wb')\n",
    "for obj in [train2, val2, test2]:\n",
    "    cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...,  158  211  327]\n",
      " [   0    0    0 ...,  392  432  421]\n",
      " [  21 1547  605 ...,  939 1181  506]\n",
      " [4268 1060   76 ...,  206   86  327]\n",
      " [   0    0    0 ...,  889 1204 1362]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "\n",
    "datafile = \"./data/DATA_WORDSEQV1_HADM_TOP10CAT.p\"\n",
    "f = open(datafile, 'rb')\n",
    "loaded_data = []\n",
    "for i in range(7): # [reverse_dictionary, train_sequence, test_sequence, train_label, test_label]:\n",
    "    loaded_data.append(cPickle.load(f))\n",
    "f.close()\n",
    "\n",
    "dictionary = loaded_data[0]\n",
    "train_sequence = loaded_data[1]\n",
    "val_sequence = loaded_data[2]\n",
    "test_sequence = loaded_data[3]\n",
    "train_label = loaded_data[4]\n",
    "val_label = loaded_data[5]\n",
    "test_label = loaded_data[6]\n",
    "\n",
    "print train_sequence[:5,:]\n",
    "print train_label[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
