{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Base Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors, MLUtils\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    # https://spark.apache.org/docs/latest/ml-migration-guides.html\n",
    "    new_df = MLUtils.convertVectorColumnsToML(df.withColumn('features', udf(df.features)))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "concat_udf = F.udf(lambda cols: float(int(\"\".join([str(int(x)) for x in cols]), 2)), DoubleType())\n",
    "\n",
    "def evaluate(df, labelCols):\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df.cache()\n",
    "    \n",
    "    r_list = {i: [] for i in ['accuracy', 'precision', 'recall', 'fmeasure']}\n",
    "    for i in xrange(len(labelCols)):\n",
    "        predandlabels = df.select(labelCols2[i], labelCols[i]).rdd \\\n",
    "                        .map(lambda x: (float(x[labelCols2[i]]), float(x[labelCols[i]])))\n",
    "        metrics = MulticlassMetrics(predandlabels)\n",
    "\n",
    "        # print metrics.confusionMatrix()\n",
    "        r_list['accuracy'].append(metrics.accuracy)\n",
    "        r_list['precision'].append(metrics.precision(1.0))\n",
    "        r_list['recall'].append(metrics.recall(1.0))\n",
    "        r_list['fmeasure'].append(metrics.fMeasure(label=1.0))\n",
    "\n",
    "    results = {m: (sum(rs) / len(rs)) for (m, rs) in r_list.iteritems()}\n",
    "            \n",
    "    return results\n",
    "\n",
    "def evaluate_em(df, labelCols, metrics=[\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]):\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df2 = df.withColumn(\"_label\", concat_udf(F.array(labelCols)))\n",
    "    df2 = df2.withColumn(\"_pred\", concat_udf(F.array(labelCols2)))\n",
    "    \n",
    "    output = {}\n",
    "    for m in metrics:\n",
    "        result = evaluator.evaluate(df2, {evaluator.metricName: m,\n",
    "                                         evaluator.predictionCol: \"_pred\",\n",
    "                                         evaluator.labelCol: \"_label\"})\n",
    "        output[m] = result\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxIter=100, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = LogisticRegression(featuresCol=featuresCol,\n",
    "                                    labelCol=c,\n",
    "                                    predictionCol=c+\"_pred\",\n",
    "                                    probabilityCol=c+\"_prob\",\n",
    "                                    rawPredictionCol=c+\"_rpred\",\n",
    "                                    maxIter=maxIter,\n",
    "                                    regParam=regParam,\n",
    "                                    family=\"binomial\")\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "class CustomRandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxDepth=5, maxBins=32, numTrees=20, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = RandomForestClassifier(featuresCol=featuresCol,\n",
    "                                        labelCol=c,\n",
    "                                        predictionCol=c+\"_pred\",\n",
    "                                        probabilityCol=c+\"_prob\",\n",
    "                                        rawPredictionCol=c+\"_rpred\",\n",
    "                                        maxDepth=maxDepth,\n",
    "                                        maxBins=maxBins,\n",
    "                                        impurity=\"gini\",\n",
    "                                        numTrees=numTrees,\n",
    "                                        seed=None)\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_latex(inum, m1, m2, m3, m4):\n",
    "    r1 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m1)\n",
    "    r2 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m2)\n",
    "    r3 = \"{accuracy:.4f}\".format(**m3)\n",
    "    r4 = \"{accuracy:.4f}\".format(**m4)\n",
    "    return \"{0} & {1} & {2} & {3} & {4} \\\\\\\\ \\hline\".format(inum, r1, r3, r2, r4)\n",
    "    \n",
    "def run_experiment(input_name):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    #df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"\n",
    "    for maxIter in [5, 10, 25, 50, 75, 100]:\n",
    "        clr = CustomLogisticRegression()\n",
    "        clr.fit(df_train, maxIter=maxIter)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxIter, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment2(input_name, depths=[5, 10, 20, 30]):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    #df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"        \n",
    "    for maxDepth in depths:\n",
    "        clr = CustomRandomForestClassifier()\n",
    "        clr.fit(df_train, maxDepth=maxDepth)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxDepth, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-f816a37e-bcdc-4186-8e76-863d312765fe/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "  File \"<string>\", line 18, in run_experiment\n",
       "  File \"<string>\", line 21, in fit\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/pyspark/ml/base.py\", line 64, in fit\n",
       "    return self._fit(dataset)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 236, in _fit\n",
       "    java_model = self._fit_java(dataset)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 233, in _fit_java\n",
       "    return self._java_obj.fit(dataset._jdf)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n",
       "    answer = self.gateway_client.send_command(command)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n",
       "    response = connection.send_command(command)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n",
       "    answer = smart_decode(self.stream.readline()[:-1])\n",
       "  File \"/home/docker-user/anaconda2/envs/cse6250/lib/python2.7/socket.py\", line 451, in readline\n",
       "    data = self._sock.recv(self._rbufsize)\n",
       "  File \"/home/docker-user/cse6250-final-project/spark-2.1.0-bin-hadoop2.7/python/pyspark/context.py\", line 236, in signal_handler\n",
       "    raise KeyboardInterrupt()\n",
       "KeyboardInterrupt\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: null was reset!\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$reset$1.apply(BrokerState.scala:191)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$reset$1.apply(BrokerState.scala:189)\n",
       "scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n",
       "org.apache.toree.interpreter.broker.BrokerState.reset(BrokerState.scala:189)\n",
       "org.apache.toree.kernel.interpreter.pyspark.PySparkService$$anonfun$pySparkProcess$2.apply(PySparkService.scala:63)\n",
       "org.apache.toree.kernel.interpreter.pyspark.PySparkService$$anonfun$pySparkProcess$2.apply(PySparkService.scala:61)\n",
       "org.apache.toree.interpreter.broker.BrokerProcessHandler.onProcessComplete(BrokerProcessHandler.scala:67)\n",
       "org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:201)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV1_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-f816a37e-bcdc-4186-8e76-863d312765fe/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "NameError: name 'run_experiment' is not defined\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_experiment2(\"./data/DATA_WORD2VEC_HADM_TOP50\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP50\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
