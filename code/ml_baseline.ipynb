{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Base Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors, MLUtils\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    # https://spark.apache.org/docs/latest/ml-migration-guides.html\n",
    "    new_df = MLUtils.convertVectorColumnsToML(df.withColumn('features', udf(df.features)))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 30430\n",
      "Test: 10132\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|100050|   0|   0|    0|   1|    1|    1|    0|    1|   0|   1|(40000,[69,78,104...|\n",
      "|100053|   0|   0|    1|   0|    0|    0|    0|    1|   0|   0|(40000,[794,2044,...|\n",
      "|100059|   1|   0|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[130,207,3...|\n",
      "|100061|   0|   0|    0|   1|    0|    0|    0|    0|   1|   0|(40000,[24,151,20...|\n",
      "|100065|   1|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[115,794,8...|\n",
      "|100066|   1|   0|    0|   0|    1|    1|    0|    0|   0|   0|(40000,[130,184,1...|\n",
      "|100074|   0|   0|    0|   0|    0|    0|    1|    0|   0|   0|(40000,[32,62,71,...|\n",
      "|100077|   0|   1|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[78,102,12...|\n",
      "|100281|   0|   0|    0|   1|    1|    0|    0|    1|   0|   0|(40000,[30,48,78,...|\n",
      "|100282|   0|   0|    0|   1|    0|    0|    0|    0|   1|   0|(40000,[379,585,7...|\n",
      "|100283|   1|   0|    1|   0|    0|    0|    1|    1|   0|   0|(40000,[32,43,162...|\n",
      "|100284|   0|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[312,794,1...|\n",
      "|100294|   1|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[207,574,5...|\n",
      "|100295|   0|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[207,251,5...|\n",
      "|100297|   1|   1|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[69,189,79...|\n",
      "|100298|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[63,207,29...|\n",
      "|100300|   0|   1|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[1,200,207...|\n",
      "|100302|   0|   0|    0|   0|    0|    1|    1|    0|   0|   0|(40000,[130,168,2...|\n",
      "|100303|   0|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[71,130,20...|\n",
      "|100307|   1|   0|    0|   0|    1|    0|    0|    0|   0|   1|(40000,[32,115,20...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|100063|   1|   0|    0|   0|    0|    0|    1|    0|   0|   0|(40000,[115,187,2...|\n",
      "|100068|   0|   1|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[1,20,21,6...|\n",
      "|100071|   0|   0|    0|   0|    0|    1|    0|    0|   0|   0|(40000,[10,32,78,...|\n",
      "|100289|   0|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[228,574,1...|\n",
      "|100290|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[1,20,115,...|\n",
      "|100292|   0|   1|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[115,207,5...|\n",
      "|100538|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[48,207,29...|\n",
      "|100749|   1|   1|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[35,122,12...|\n",
      "|100754|   0|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[20,32,48,...|\n",
      "|100974|   0|   0|    1|   0|    0|    0|    0|    0|   0|   0|(40000,[794,890,1...|\n",
      "|100978|   1|   1|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[574,794,1...|\n",
      "|100990|   0|   0|    0|   1|    0|    0|    0|    1|   1|   0|(40000,[138,574,6...|\n",
      "|101208|   0|   0|    0|   1|    0|    0|    0|    1|   0|   0|(40000,[151,273,5...|\n",
      "|101220|   1|   0|    1|   0|    1|    0|    0|    1|   0|   0|(40000,[379,697,7...|\n",
      "|101227|   0|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[32,48,78,...|\n",
      "|101432|   1|   1|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[6,9,26,27...|\n",
      "|101433|   1|   0|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[32,104,20...|\n",
      "|101440|   1|   0|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[794,848,1...|\n",
      "|101441|   1|   0|    0|   0|    0|    0|    0|    0|   1|   1|(40000,[10,122,30...|\n",
      "|101675|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[130,190,2...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = read_csv(\"./data/DATA_TFIDF_HADM_TOP10.csv\")\n",
    "df_train, df_test = df.randomSplit(weights=[0.75, 0.25], seed=12345)\n",
    "df_train.cache()\n",
    "df_test.cache()\n",
    "print \"Train:\", df_train.count()\n",
    "print \"Test:\", df_test.count()\n",
    "df_train.show()\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "concat_udf = F.udf(lambda cols: float(int(\"\".join([str(int(x)) for x in cols]), 2)), DoubleType())\n",
    "\n",
    "def evaluate(df, labelCols, metrics=[\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]):\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df2 = df.withColumn(\"_label\", concat_udf(F.array(labelCols)))\n",
    "    df2 = df2.withColumn(\"_pred\", concat_udf(F.array(labelCols2)))\n",
    "    \n",
    "    output = {}\n",
    "    for m in metrics:\n",
    "        result = evaluator.evaluate(df2, {evaluator.metricName: m,\n",
    "                                         evaluator.predictionCol: \"_pred\",\n",
    "                                         evaluator.labelCol: \"_label\"})\n",
    "        output[m] = result\n",
    "        \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxIter=100, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = LogisticRegression(featuresCol=featuresCol,\n",
    "                                    labelCol=c,\n",
    "                                    predictionCol=c+\"_pred\",\n",
    "                                    probabilityCol=c+\"_prob\",\n",
    "                                    rawPredictionCol=c+\"_rpred\",\n",
    "                                    maxIter=maxIter,\n",
    "                                    regParam=regParam,\n",
    "                                    family=\"binomial\")\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clr = CustomLogisticRegression()\n",
    "clr.fit(df_train)\n",
    "df_pred_train = clr.predict(df_train)\n",
    "df_pred_test = clr.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxIter:  5\n",
      "{'weightedPrecision': 0.3191686583865087, 'f1': 0.2422791139786058, 'weightedRecall': 0.25681892868879336, 'accuracy': 0.25681892868879397}\n",
      "{'weightedPrecision': 0.10625387820800844, 'f1': 0.09190509299831698, 'weightedRecall': 0.12258191867350976, 'accuracy': 0.12258191867350968}\n",
      "maxIter:  10\n",
      "{'weightedPrecision': 0.7841250725433033, 'f1': 0.7620669774918355, 'weightedRecall': 0.7511994742030924, 'accuracy': 0.7511994742030891}\n",
      "{'weightedPrecision': 0.12330947229469667, 'f1': 0.11949211928027403, 'weightedRecall': 0.12425977102250312, 'accuracy': 0.12425977102250296}\n",
      "maxIter:  25\n",
      "{'weightedPrecision': 0.9999712454814376, 'f1': 0.9999675758571632, 'weightedRecall': 0.9999671376930708, 'accuracy': 0.999967137693066}\n",
      "{'weightedPrecision': 0.10715632789997416, 'f1': 0.10053642873404066, 'weightedRecall': 0.09800631662060805, 'accuracy': 0.09800631662060798}\n",
      "maxIter:  50\n",
      "{'weightedPrecision': 1.0000000000000049, 'f1': 1.0000000000000049, 'weightedRecall': 1.0000000000000049, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10566124747545828, 'f1': 0.09863138825157139, 'weightedRecall': 0.09632846427161476, 'accuracy': 0.09632846427161469}\n",
      "maxIter:  75\n",
      "{'weightedPrecision': 1.0000000000000049, 'f1': 1.0000000000000049, 'weightedRecall': 1.0000000000000049, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10566124747545828, 'f1': 0.09863138825157139, 'weightedRecall': 0.09632846427161476, 'accuracy': 0.09632846427161469}\n",
      "maxIter:  100\n",
      "{'weightedPrecision': 1.0000000000000049, 'f1': 1.0000000000000049, 'weightedRecall': 1.0000000000000049, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10566124747545828, 'f1': 0.09863138825157139, 'weightedRecall': 0.09632846427161476, 'accuracy': 0.09632846427161469}\n"
     ]
    }
   ],
   "source": [
    "for maxIter in [5, 10, 25, 50, 75, 100]:\n",
    "    clr = CustomLogisticRegression()\n",
    "    clr.fit(df_train, maxIter=maxIter)\n",
    "    df_pred_train = clr.predict(df_train)\n",
    "    df_pred_test = clr.predict(df_test)\n",
    "\n",
    "    print \"maxIter: \", maxIter\n",
    "    print evaluate(df_pred_train, clr.labelCols)\n",
    "    print evaluate(df_pred_test, clr.labelCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "class CustomRandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxDepth=5, maxBins=32, numTrees=20, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = RandomForestClassifier(featuresCol=featuresCol,\n",
    "                                        labelCol=c,\n",
    "                                        predictionCol=c+\"_pred\",\n",
    "                                        probabilityCol=c+\"_prob\",\n",
    "                                        rawPredictionCol=c+\"_rpred\",\n",
    "                                        maxDepth=maxDepth,\n",
    "                                        maxBins=maxBins,\n",
    "                                        impurity=\"gini\",\n",
    "                                        numTrees=numTrees,\n",
    "                                        seed=None)\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for maxDepth in [5, 10, 25, 50, 75, 100]:\n",
    "    clr = CustomRandomForestClassifier()\n",
    "    clr.fit(df_train, maxDepth=maxDepth)\n",
    "    df_pred_train = clr.predict(df_train)\n",
    "    df_pred_test = clr.predict(df_test)\n",
    "\n",
    "    print \"maxDepth: \", maxDepth\n",
    "    print evaluate(df_pred_train, clr.labelCols)\n",
    "    print evaluate(df_pred_test, clr.labelCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
