{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40562\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|117760|   0|   0|    0|   0|    0|    1|    1|    0|   0|   0|(40000,[69,372,69...|\n",
      "|129030|   1|   1|    0|   0|    0|    1|    0|    0|   0|   0|(40000,[13,32,83,...|\n",
      "|172040|   0|   0|    0|   0|    1|    0|    0|    0|   1|   0|(40000,[10,69,152...|\n",
      "|156170|   0|   0|    1|   1|    0|    0|    0|    1|   1|   0|(40000,[3,78,130,...|\n",
      "|199180|   0|   0|    1|   1|    1|    0|    0|    0|   0|   0|(40000,[48,62,80,...|\n",
      "|167440|   0|   0|    1|   0|    1|    0|    1|    0|   0|   0|(40000,[207,264,2...|\n",
      "|178710|   0|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[574,794,1...|\n",
      "|162840|   0|   1|    0|   1|    0|    0|    0|    1|   0|   0|(40000,[32,130,15...|\n",
      "|189980|   1|   0|    1|   1|    0|    0|    0|    0|   0|   0|(40000,[75,207,33...|\n",
      "|142370|   0|   0|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[22,30,207...|\n",
      "|153640|   0|   0|    0|   0|    1|    0|    1|    0|   0|   0|(40000,[62,63,85,...|\n",
      "|180780|   0|   0|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[207,648,7...|\n",
      "|149040|   1|   0|    0|   0|    1|    0|    1|    0|   0|   0|(40000,[574,794,1...|\n",
      "|101430|   1|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[33,63,71,...|\n",
      "|132020|   1|   1|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[130,187,2...|\n",
      "|155710|   1|   0|    0|   1|    0|    1|    1|    0|   1|   0|(40000,[63,92,115...|\n",
      "|151110|   0|   0|    0|   0|    0|    0|    0|    1|   1|   0|(40000,[63,73,169...|\n",
      "|140300|   0|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[37,83,106...|\n",
      "|103500|   1|   0|    1|   0|    1|    0|    0|    1|   0|   0|(40000,[12,35,130...|\n",
      "|130640|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[207,312,5...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdf = read_csv(\"./data/DATA_TFIDF_HADM_TOP10.csv\")\n",
    "print testdf.count()\n",
    "testdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
