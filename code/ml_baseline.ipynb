{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Base Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1490841303863_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-cse625.2111020eht4ehjujlvoh0esvde.bx.internal.cloudapp.net:8088/proxy/application_1490841303863_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.5:30060/node/containerlogs/container_1490841303863_0005_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors, MLUtils\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    # https://spark.apache.org/docs/latest/ml-migration-guides.html\n",
    "    new_df = MLUtils.convertVectorColumnsToML(df.withColumn('features', udf(df.features)))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 30400\n",
      "Test: 10162\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|100050|   0|   0|    0|   1|    1|    1|    0|    1|   0|   1|(40000,[69,78,104...|\n",
      "|100053|   0|   0|    1|   0|    0|    0|    0|    1|   0|   0|(40000,[794,2044,...|\n",
      "|100059|   1|   0|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[130,207,3...|\n",
      "|100061|   0|   0|    0|   1|    0|    0|    0|    0|   1|   0|(40000,[24,151,20...|\n",
      "|100281|   0|   0|    0|   1|    1|    0|    0|    1|   0|   0|(40000,[30,48,78,...|\n",
      "|100282|   0|   0|    0|   1|    0|    0|    0|    0|   1|   0|(40000,[379,585,7...|\n",
      "|100289|   0|   0|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[228,574,1...|\n",
      "|100290|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[1,20,115,...|\n",
      "|100292|   0|   1|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[115,207,5...|\n",
      "|100294|   1|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[207,574,5...|\n",
      "|100511|   1|   0|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[207,273,2...|\n",
      "|100517|   1|   1|    1|   1|    0|    0|    0|    1|   0|   0|(40000,[48,63,71,...|\n",
      "|100742|   0|   1|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[1,48,104,...|\n",
      "|100746|   1|   0|    0|   0|    0|    0|    0|    0|   0|   1|(40000,[794,954,1...|\n",
      "|100747|   0|   1|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[187,207,2...|\n",
      "|100748|   1|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[135,273,2...|\n",
      "|100749|   1|   1|    0|   0|    0|    0|    1|    0|   1|   0|(40000,[35,122,12...|\n",
      "|100753|   0|   0|    1|   1|    0|    0|    1|    0|   0|   0|(40000,[462,600,7...|\n",
      "|100754|   0|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[20,32,48,...|\n",
      "|100974|   0|   0|    1|   0|    0|    0|    0|    0|   0|   0|(40000,[794,890,1...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|    id|4019|2724|25000|4280|41401|53081|51881|42731|5849|5990|            features|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "|100063|   1|   0|    0|   0|    0|    0|    1|    0|   0|   0|(40000,[115,187,2...|\n",
      "|100283|   1|   0|    1|   0|    0|    0|    1|    1|   0|   0|(40000,[32,43,162...|\n",
      "|100284|   0|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[312,794,1...|\n",
      "|100522|   0|   0|    0|   1|    0|    1|    0|    0|   0|   1|(40000,[32,71,73,...|\n",
      "|100524|   0|   0|    0|   0|    0|    0|    1|    1|   0|   0|(40000,[69,112,12...|\n",
      "|100525|   0|   0|    0|   1|    0|    0|    1|    1|   0|   0|(40000,[32,100,20...|\n",
      "|101211|   0|   0|    0|   1|    0|    0|    0|    0|   0|   1|(40000,[78,92,122...|\n",
      "|101440|   1|   0|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[794,848,1...|\n",
      "|101661|   0|   0|    0|   0|    1|    0|    0|    0|   0|   0|(40000,[196,207,2...|\n",
      "|101902|   1|   1|    1|   0|    1|    1|    0|    0|   0|   0|(40000,[379,676,7...|\n",
      "|101903|   1|   0|    0|   0|    0|    0|    0|    0|   0|   0|(40000,[152,794,2...|\n",
      "|102127|   0|   1|    1|   0|    1|    0|    0|    0|   0|   0|(40000,[69,273,35...|\n",
      "|102359|   0|   0|    0|   1|    1|    0|    0|    0|   0|   0|(40000,[60,233,46...|\n",
      "|102589|   0|   0|    0|   0|    0|    0|    0|    1|   0|   0|(40000,[62,78,91,...|\n",
      "|102821|   0|   1|    0|   0|    0|    0|    0|    0|   1|   0|(40000,[32,128,20...|\n",
      "|103041|   1|   0|    0|   0|    0|    0|    1|    1|   1|   0|(40000,[794,1004,...|\n",
      "|103045|   1|   0|    0|   1|    0|    0|    0|    1|   1|   0|(40000,[71,115,57...|\n",
      "|103050|   0|   0|    1|   0|    0|    0|    0|    0|   0|   0|(40000,[48,103,13...|\n",
      "|103053|   0|   0|    0|   0|    0|    0|    1|    0|   0|   0|(40000,[21,69,100...|\n",
      "|103511|   0|   0|    0|   0|    0|    0|    1|    0|   0|   0|(40000,[75,85,122...|\n",
      "+------+----+----+-----+----+-----+-----+-----+-----+----+----+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df = read_csv(\"/HdiNotebooks/DATA_TFIDF_HADM_TOP10.csv\")\n",
    "df_train, df_test = df.randomSplit(weights=[0.75, 0.25], seed=12345)\n",
    "df_train.cache()\n",
    "df_test.cache()\n",
    "print \"Train:\", df_train.count()\n",
    "print \"Test:\", df_test.count()\n",
    "df_train.show()\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "concat_udf = F.udf(lambda cols: float(int(\"\".join([str(int(x)) for x in cols]), 2)), DoubleType())\n",
    "\n",
    "def evaluate(df, labelCols, metrics=[\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]):\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df2 = df.withColumn(\"_label\", concat_udf(F.array(labelCols)))\n",
    "    df2 = df2.withColumn(\"_pred\", concat_udf(F.array(labelCols2)))\n",
    "    \n",
    "    output = {}\n",
    "    for m in metrics:\n",
    "        result = evaluator.evaluate(df2, {evaluator.metricName: m,\n",
    "                                         evaluator.predictionCol: \"_pred\",\n",
    "                                         evaluator.labelCol: \"_label\"})\n",
    "        output[m] = result\n",
    "        \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxIter=100, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = LogisticRegression(featuresCol=featuresCol,\n",
    "                                    labelCol=c,\n",
    "                                    predictionCol=c+\"_pred\",\n",
    "                                    probabilityCol=c+\"_prob\",\n",
    "                                    rawPredictionCol=c+\"_rpred\",\n",
    "                                    maxIter=maxIter,\n",
    "                                    regParam=regParam,\n",
    "                                    family=\"binomial\")\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxIter:  5\n",
      "{'weightedPrecision': 0.32422971544867885, 'f1': 0.2487607767288541, 'weightedRecall': 0.26467105263157803, 'accuracy': 0.264671052631579}\n",
      "{'weightedPrecision': 0.10921601442253366, 'f1': 0.09381365749094564, 'weightedRecall': 0.12595945679984275, 'accuracy': 0.12595945679984255}\n",
      "maxIter:  10\n",
      "{'weightedPrecision': 0.7887577146619332, 'f1': 0.7696060340197072, 'weightedRecall': 0.7609539473684176, 'accuracy': 0.760953947368421}\n",
      "{'weightedPrecision': 0.1290803781671321, 'f1': 0.1253548498061775, 'weightedRecall': 0.13225742963983492, 'accuracy': 0.13225742963983467}\n",
      "maxIter:  25\n",
      "{'weightedPrecision': 0.9998710805070459, 'f1': 0.9998681263464682, 'weightedRecall': 0.9998684210526279, 'accuracy': 0.9998684210526316}\n",
      "{'weightedPrecision': 0.10314755726388049, 'f1': 0.09796554367340238, 'weightedRecall': 0.09781539067112782, 'accuracy': 0.09781539067112772}\n",
      "maxIter:  50\n",
      "{'weightedPrecision': 0.9999999999999962, 'f1': 0.9999999999999962, 'weightedRecall': 0.9999999999999962, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10355121204655478, 'f1': 0.09825534639138672, 'weightedRecall': 0.09781539067112782, 'accuracy': 0.09781539067112772}\n",
      "maxIter:  75\n",
      "{'weightedPrecision': 0.9999999999999962, 'f1': 0.9999999999999962, 'weightedRecall': 0.9999999999999962, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10355121204655478, 'f1': 0.09825534639138672, 'weightedRecall': 0.09781539067112782, 'accuracy': 0.09781539067112772}\n",
      "maxIter:  100\n",
      "{'weightedPrecision': 0.9999999999999962, 'f1': 0.9999999999999962, 'weightedRecall': 0.9999999999999962, 'accuracy': 1.0}\n",
      "{'weightedPrecision': 0.10355121204655478, 'f1': 0.09825534639138672, 'weightedRecall': 0.09781539067112782, 'accuracy': 0.09781539067112772}"
     ]
    }
   ],
   "source": [
    "for maxIter in [5, 10, 25, 50, 75, 100]:\n",
    "    clr = CustomLogisticRegression()\n",
    "    clr.fit(df_train, maxIter=maxIter)\n",
    "    df_pred_train = clr.predict(df_train)\n",
    "    df_pred_test = clr.predict(df_test)\n",
    "\n",
    "    print \"maxIter: \", maxIter\n",
    "    print evaluate(df_pred_train, clr.labelCols)\n",
    "    print evaluate(df_pred_test, clr.labelCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "class CustomRandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxDepth=5, maxBins=32, numTrees=20, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = RandomForestClassifier(featuresCol=featuresCol,\n",
    "                                        labelCol=c,\n",
    "                                        predictionCol=c+\"_pred\",\n",
    "                                        probabilityCol=c+\"_prob\",\n",
    "                                        rawPredictionCol=c+\"_rpred\",\n",
    "                                        maxDepth=maxDepth,\n",
    "                                        maxBins=maxBins,\n",
    "                                        impurity=\"gini\",\n",
    "                                        numTrees=numTrees,\n",
    "                                        seed=None)\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxDepth:  5\n",
      "{'weightedPrecision': 0.06003821647001132, 'f1': 0.02490492183351698, 'weightedRecall': 0.06743421052631579, 'accuracy': 0.06743421052631579}\n",
      "{'weightedPrecision': 0.06822878344400911, 'f1': 0.022155707150682047, 'weightedRecall': 0.06366856917929542, 'accuracy': 0.06366856917929542}\n",
      "maxDepth:  10\n",
      "{'weightedPrecision': 0.13480532836332554, 'f1': 0.06533599216963552, 'weightedRecall': 0.10861842105263164, 'accuracy': 0.10861842105263157}\n",
      "{'weightedPrecision': 0.07147709071387036, 'f1': 0.04029352952448755, 'weightedRecall': 0.08364495178114549, 'accuracy': 0.08364495178114545}\n",
      "maxDepth:  20\n",
      "{'weightedPrecision': 0.4457382831587539, 'f1': 0.2741693567900213, 'weightedRecall': 0.29098684210526216, 'accuracy': 0.29098684210526315}\n",
      "{'weightedPrecision': 0.08816127543442014, 'f1': 0.06109304953121328, 'weightedRecall': 0.10096437709112385, 'accuracy': 0.1009643770911238}\n",
      "maxDepth:  30\n",
      "{'weightedPrecision': 0.6360990909870188, 'f1': 0.48942733688808804, 'weightedRecall': 0.48480263157894515, 'accuracy': 0.4848026315789474}\n",
      "{'weightedPrecision': 0.10370873560138762, 'f1': 0.07552076371132027, 'weightedRecall': 0.11070655382798665, 'accuracy': 0.11070655382798662}"
     ]
    }
   ],
   "source": [
    "for maxDepth in [5, 10, 20, 30]:\n",
    "    clr = CustomRandomForestClassifier()\n",
    "    clr.fit(df_train, maxDepth=maxDepth)\n",
    "    df_pred_train = clr.predict(df_train)\n",
    "    df_pred_test = clr.predict(df_test)\n",
    "\n",
    "    print \"maxDepth: \", maxDepth\n",
    "    print evaluate(df_pred_train, clr.labelCols)\n",
    "    print evaluate(df_pred_test, clr.labelCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}