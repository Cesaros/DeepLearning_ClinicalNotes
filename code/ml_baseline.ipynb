{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Base Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors, MLUtils\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    # https://spark.apache.org/docs/latest/ml-migration-guides.html\n",
    "    new_df = MLUtils.convertVectorColumnsToML(df.withColumn('features', udf(df.features)))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "concat_udf = F.udf(lambda cols: float(int(\"\".join([str(int(x)) for x in cols]), 2)), DoubleType())\n",
    "\n",
    "def evaluate(df, labelCols):\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df.cache()\n",
    "    \n",
    "    r_list = {i: [] for i in ['accuracy', 'precision', 'recall', 'fmeasure']}\n",
    "    for i in xrange(len(labelCols)):\n",
    "        predandlabels = df.select(labelCols2[i], labelCols[i]).rdd \\\n",
    "                        .map(lambda x: (float(x[labelCols2[i]]), float(x[labelCols[i]])))\n",
    "        metrics = MulticlassMetrics(predandlabels)\n",
    "\n",
    "        # print metrics.confusionMatrix()\n",
    "        r_list['accuracy'].append(metrics.accuracy)\n",
    "        r_list['precision'].append(metrics.precision(1.0))\n",
    "        r_list['recall'].append(metrics.recall(1.0))\n",
    "        r_list['fmeasure'].append(metrics.fMeasure(label=1.0))\n",
    "\n",
    "    results = {m: (sum(rs) / len(rs)) for (m, rs) in r_list.iteritems()}\n",
    "            \n",
    "    return results\n",
    "\n",
    "def evaluate_em(df, labelCols, metrics=[\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]):\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df2 = df.withColumn(\"_label\", concat_udf(F.array(labelCols)))\n",
    "    df2 = df2.withColumn(\"_pred\", concat_udf(F.array(labelCols2)))\n",
    "    \n",
    "    output = {}\n",
    "    for m in metrics:\n",
    "        result = evaluator.evaluate(df2, {evaluator.metricName: m,\n",
    "                                         evaluator.predictionCol: \"_pred\",\n",
    "                                         evaluator.labelCol: \"_label\"})\n",
    "        output[m] = result\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxIter=100, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = LogisticRegression(featuresCol=featuresCol,\n",
    "                                    labelCol=c,\n",
    "                                    predictionCol=c+\"_pred\",\n",
    "                                    probabilityCol=c+\"_prob\",\n",
    "                                    rawPredictionCol=c+\"_rpred\",\n",
    "                                    maxIter=maxIter,\n",
    "                                    regParam=regParam,\n",
    "                                    family=\"binomial\")\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "class CustomRandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxDepth=5, maxBins=32, numTrees=20, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = RandomForestClassifier(featuresCol=featuresCol,\n",
    "                                        labelCol=c,\n",
    "                                        predictionCol=c+\"_pred\",\n",
    "                                        probabilityCol=c+\"_prob\",\n",
    "                                        rawPredictionCol=c+\"_rpred\",\n",
    "                                        maxDepth=maxDepth,\n",
    "                                        maxBins=maxBins,\n",
    "                                        impurity=\"gini\",\n",
    "                                        numTrees=numTrees,\n",
    "                                        seed=None)\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_latex(inum, m1, m2, m3, m4):\n",
    "    r1 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m1)\n",
    "    r2 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m2)\n",
    "    r3 = \"{accuracy:.4f}\".format(**m3)\n",
    "    r4 = \"{accuracy:.4f}\".format(**m4)\n",
    "    return \"{0} & {1} & {2} & {3} & {4} \\\\\\\\ \\hline\".format(inum, r1, r3, r2, r4)\n",
    "    \n",
    "def run_experiment(input_name):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"\n",
    "    for maxIter in [5, 10, 25, 50, 75, 100]:\n",
    "        clr = CustomLogisticRegression()\n",
    "        clr.fit(df_train, maxIter=maxIter)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxIter, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment2(input_name):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"        \n",
    "    for maxDepth in [5, 10, 20, 30]:\n",
    "        clr = CustomRandomForestClassifier()\n",
    "        clr.fit(df_train, maxDepth=maxDepth)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxDepth, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.8678 & 0.4640 & 0.5905 & 0.8848 & 0.3809 & 0.6756 & 0.3032 & 0.4036 & 0.8417 & 0.2704 \\\\ \\hline\n",
      "10 & 0.9602 & 0.8912 & 0.9241 & 0.9701 & 0.7916 & 0.5964 & 0.4561 & 0.5138 & 0.8411 & 0.2558 \\\\ \\hline\n",
      "25 & 0.9998 & 0.9999 & 0.9999 & 0.9999 & 0.9991 & 0.5208 & 0.4554 & 0.4844 & 0.8206 & 0.2189 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5165 & 0.4516 & 0.4804 & 0.8190 & 0.2176 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5165 & 0.4515 & 0.4804 & 0.8190 & 0.2176 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5165 & 0.4515 & 0.4804 & 0.8190 & 0.2176 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.8060 & 0.5491 & 0.6508 & 0.8915 & 0.3833 & 0.6689 & 0.4006 & 0.4977 & 0.8513 & 0.2847 \\\\ \\hline\n",
      "10 & 0.9415 & 0.9169 & 0.9289 & 0.9711 & 0.7685 & 0.5805 & 0.4995 & 0.5358 & 0.8396 & 0.2607 \\\\ \\hline\n",
      "25 & 0.9994 & 0.9994 & 0.9994 & 0.9996 & 0.9960 & 0.5050 & 0.4827 & 0.4933 & 0.8150 & 0.2108 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.4976 & 0.4808 & 0.4889 & 0.8122 & 0.2073 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.4974 & 0.4807 & 0.4887 & 0.8121 & 0.2069 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.4974 & 0.4807 & 0.4887 & 0.8121 & 0.2069 \\\\ \\hline\n",
      "./data/DATA_WORD2VEC_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.5080 & 0.2077 & 0.2634 & 0.8255 & 0.2429 & 0.5061 & 0.2086 & 0.2632 & 0.8242 & 0.2400 \\\\ \\hline\n",
      "10 & 0.5911 & 0.2475 & 0.3186 & 0.8329 & 0.2516 & 0.5678 & 0.2455 & 0.3151 & 0.8313 & 0.2458 \\\\ \\hline\n",
      "25 & 0.6069 & 0.2688 & 0.3416 & 0.8364 & 0.2581 & 0.5759 & 0.2646 & 0.3352 & 0.8340 & 0.2519 \\\\ \\hline\n",
      "50 & 0.6067 & 0.2685 & 0.3417 & 0.8365 & 0.2581 & 0.5770 & 0.2653 & 0.3363 & 0.8342 & 0.2525 \\\\ \\hline\n",
      "75 & 0.6066 & 0.2689 & 0.3421 & 0.8364 & 0.2585 & 0.5773 & 0.2657 & 0.3371 & 0.8343 & 0.2530 \\\\ \\hline\n",
      "100 & 0.6079 & 0.2689 & 0.3424 & 0.8365 & 0.2592 & 0.5777 & 0.2659 & 0.3374 & 0.8345 & 0.2536 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VEC_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.4722 & 0.0172 & 0.0324 & 0.8076 & 0.2329 & 0.3556 & 0.0166 & 0.0310 & 0.8062 & 0.2313 \\\\ \\hline\n",
      "10 & 0.8717 & 0.1211 & 0.1952 & 0.8311 & 0.2566 & 0.6316 & 0.0842 & 0.1357 & 0.8201 & 0.2440 \\\\ \\hline\n",
      "20 & 0.9924 & 0.3686 & 0.5038 & 0.8860 & 0.3940 & 0.7632 & 0.1571 & 0.2306 & 0.8316 & 0.2581 \\\\ \\hline\n",
      "30 & 0.9974 & 0.5644 & 0.6957 & 0.9258 & 0.5583 & 0.7462 & 0.1931 & 0.2725 & 0.8366 & 0.2654 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.4706 & 0.0327 & 0.0568 & 0.8112 & 0.2356 & 0.4658 & 0.0321 & 0.0557 & 0.8099 & 0.2350 \\\\ \\hline\n",
      "10 & 0.9710 & 0.1340 & 0.2075 & 0.8341 & 0.2613 & 0.6449 & 0.0999 & 0.1532 & 0.8236 & 0.2469 \\\\ \\hline\n",
      "20 & 0.9939 & 0.4277 & 0.5658 & 0.8974 & 0.4252 & 0.7992 & 0.1806 & 0.2578 & 0.8369 & 0.2635 \\\\ \\hline\n",
      "30 & 0.9984 & 0.6447 & 0.7642 & 0.9404 & 0.6229 & 0.7557 & 0.2234 & 0.3070 & 0.8421 & 0.2701 \\\\ \\hline\n",
      "./data/DATA_WORD2VEC_HADM_TOP10\n",
      "Train, Test: 39544 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.3841 & 0.0766 & 0.1107 & 0.8179 & 0.2414 & 0.3883 & 0.0743 & 0.1081 & 0.8163 & 0.2403 \\\\ \\hline\n",
      "10 & 0.9258 & 0.2149 & 0.2990 & 0.8483 & 0.2905 & 0.5875 & 0.1438 & 0.2013 & 0.8253 & 0.2471 \\\\ \\hline\n",
      "20 & 0.9993 & 0.9479 & 0.9724 & 0.9916 & 0.9309 & 0.5446 & 0.1923 & 0.2591 & 0.8236 & 0.2374 \\\\ \\hline\n",
      "30 & 0.9999 & 0.9722 & 0.9858 & 0.9955 & 0.9642 & 0.5765 & 0.1758 & 0.2424 & 0.8233 & 0.2372 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VEC_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
