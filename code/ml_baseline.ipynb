{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Base Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors, MLUtils\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    # https://spark.apache.org/docs/latest/ml-migration-guides.html\n",
    "    new_df = MLUtils.convertVectorColumnsToML(df.withColumn('features', udf(df.features)))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "concat_udf = F.udf(lambda cols: float(int(\"\".join([str(int(x)) for x in cols]), 2)), DoubleType())\n",
    "\n",
    "def evaluate(df, labelCols):\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df.cache()\n",
    "    \n",
    "    r_list = {i: [] for i in ['accuracy', 'precision', 'recall', 'fmeasure']}\n",
    "    for i in xrange(len(labelCols)):\n",
    "        predandlabels = df.select(labelCols2[i], labelCols[i]).rdd \\\n",
    "                        .map(lambda x: (float(x[labelCols2[i]]), float(x[labelCols[i]])))\n",
    "        metrics = MulticlassMetrics(predandlabels)\n",
    "\n",
    "        # print metrics.confusionMatrix()\n",
    "        r_list['accuracy'].append(metrics.accuracy)\n",
    "        r_list['precision'].append(metrics.precision(1.0))\n",
    "        r_list['recall'].append(metrics.recall(1.0))\n",
    "        r_list['fmeasure'].append(metrics.fMeasure(label=1.0))\n",
    "\n",
    "    results = {m: (sum(rs) / len(rs)) for (m, rs) in r_list.iteritems()}\n",
    "            \n",
    "    return results\n",
    "\n",
    "def evaluate_em(df, labelCols, metrics=[\"f1\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\"]):\n",
    "    evaluator = MulticlassClassificationEvaluator()\n",
    "    labelCols2 = [i+\"_pred\" for i in labelCols]\n",
    "    df2 = df.withColumn(\"_label\", concat_udf(F.array(labelCols)))\n",
    "    df2 = df2.withColumn(\"_pred\", concat_udf(F.array(labelCols2)))\n",
    "    \n",
    "    output = {}\n",
    "    for m in metrics:\n",
    "        result = evaluator.evaluate(df2, {evaluator.metricName: m,\n",
    "                                         evaluator.predictionCol: \"_pred\",\n",
    "                                         evaluator.labelCol: \"_label\"})\n",
    "        output[m] = result\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxIter=100, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = LogisticRegression(featuresCol=featuresCol,\n",
    "                                    labelCol=c,\n",
    "                                    predictionCol=c+\"_pred\",\n",
    "                                    probabilityCol=c+\"_prob\",\n",
    "                                    rawPredictionCol=c+\"_rpred\",\n",
    "                                    maxIter=maxIter,\n",
    "                                    regParam=regParam,\n",
    "                                    family=\"binomial\")\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "class CustomRandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, maxDepth=5, maxBins=32, numTrees=20, regParam=0.0, featuresCol=\"features\", ignoreCols=[\"id\"]):\n",
    "        self.featuresCol = featuresCol\n",
    "        self.labelCols = df.columns\n",
    "        self.labelCols.remove(\"features\")\n",
    "        for c in ignoreCols:\n",
    "            self.labelCols.remove(c)\n",
    "        self.models = []\n",
    "        \n",
    "        for c in self.labelCols:\n",
    "            lr = RandomForestClassifier(featuresCol=featuresCol,\n",
    "                                        labelCol=c,\n",
    "                                        predictionCol=c+\"_pred\",\n",
    "                                        probabilityCol=c+\"_prob\",\n",
    "                                        rawPredictionCol=c+\"_rpred\",\n",
    "                                        maxDepth=maxDepth,\n",
    "                                        maxBins=maxBins,\n",
    "                                        impurity=\"gini\",\n",
    "                                        numTrees=numTrees,\n",
    "                                        seed=None)\n",
    "            model = lr.fit(df)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def predict(self, df):\n",
    "        df_out = df\n",
    "        for c, m in zip(self.labelCols, self.models):\n",
    "            df_out = m.transform(df_out)\n",
    "            \n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_latex(inum, m1, m2, m3, m4):\n",
    "    r1 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m1)\n",
    "    r2 = \"{precision:.4f} & {recall:.4f} & {fmeasure:.4f} & {accuracy:.4f}\".format(**m2)\n",
    "    r3 = \"{accuracy:.4f}\".format(**m3)\n",
    "    r4 = \"{accuracy:.4f}\".format(**m4)\n",
    "    return \"{0} & {1} & {2} & {3} & {4} \\\\\\\\ \\hline\".format(inum, r1, r3, r2, r4)\n",
    "    \n",
    "def run_experiment(input_name):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    #df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"\n",
    "    for maxIter in [5, 10, 25, 50, 75, 100]:\n",
    "        clr = CustomLogisticRegression()\n",
    "        clr.fit(df_train, maxIter=maxIter)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxIter, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment2(input_name, depths=[5, 10, 20, 30]):\n",
    "    df_train = read_csv(\"{0}_train.csv\".format(input_name))\n",
    "    df_val = read_csv(\"{0}_val.csv\".format(input_name))\n",
    "    df_test = read_csv(\"{0}_test.csv\".format(input_name))\n",
    "\n",
    "    #df_train = df_train.union(df_val)\n",
    "    \n",
    "    df_train.cache()\n",
    "    df_test.cache()\n",
    "    \n",
    "    print input_name\n",
    "    print \"Train, Test:\", df_train.count(), df_test.count()\n",
    "    print \"iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\"        \n",
    "    for maxDepth in depths:\n",
    "        clr = CustomRandomForestClassifier()\n",
    "        clr.fit(df_train, maxDepth=maxDepth)\n",
    "        df_pred_train = clr.predict(df_train)\n",
    "        df_pred_test = clr.predict(df_test)\n",
    "\n",
    "        r1 = evaluate(df_pred_train, clr.labelCols)\n",
    "        r2 = evaluate(df_pred_test, clr.labelCols)\n",
    "        r3 = evaluate_em(df_pred_train, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        r4 = evaluate_em(df_pred_test, clr.labelCols, metrics=[\"accuracy\"])\n",
    "        \n",
    "        print print_latex(maxDepth, r1, r2, r3, r4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.9013 & 0.5084 & 0.6389 & 0.8937 & 0.4161 & 0.6599 & 0.2864 & 0.3834 & 0.8376 & 0.2643 \\\\ \\hline\n",
      "10 & 0.9805 & 0.9332 & 0.9561 & 0.9823 & 0.8749 & 0.5850 & 0.4269 & 0.4897 & 0.8370 & 0.2499 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5290 & 0.4445 & 0.4810 & 0.8232 & 0.2243 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5262 & 0.4474 & 0.4817 & 0.8222 & 0.2236 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5262 & 0.4474 & 0.4817 & 0.8222 & 0.2236 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5262 & 0.4474 & 0.4817 & 0.8222 & 0.2236 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.8163 & 0.5689 & 0.6682 & 0.8958 & 0.3965 & 0.6642 & 0.3978 & 0.4946 & 0.8505 & 0.2835 \\\\ \\hline\n",
      "10 & 0.9564 & 0.9440 & 0.9501 & 0.9786 & 0.8202 & 0.5801 & 0.4934 & 0.5320 & 0.8392 & 0.2603 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.9999 & 0.5179 & 0.4780 & 0.4969 & 0.8192 & 0.2173 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5143 & 0.4779 & 0.4951 & 0.8179 & 0.2147 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5143 & 0.4779 & 0.4951 & 0.8179 & 0.2147 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5143 & 0.4779 & 0.4951 & 0.8179 & 0.2147 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV0_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.5087 & 0.2113 & 0.2674 & 0.8246 & 0.2408 & 0.5033 & 0.2126 & 0.2669 & 0.8238 & 0.2411 \\\\ \\hline\n",
      "10 & 0.5830 & 0.2515 & 0.3237 & 0.8325 & 0.2503 & 0.5579 & 0.2480 & 0.3165 & 0.8304 & 0.2459 \\\\ \\hline\n",
      "25 & 0.6126 & 0.2699 & 0.3432 & 0.8366 & 0.2564 & 0.5880 & 0.2659 & 0.3368 & 0.8349 & 0.2541 \\\\ \\hline\n",
      "50 & 0.6129 & 0.2731 & 0.3470 & 0.8372 & 0.2582 & 0.5820 & 0.2682 & 0.3393 & 0.8353 & 0.2542 \\\\ \\hline\n",
      "75 & 0.6144 & 0.2735 & 0.3475 & 0.8372 & 0.2580 & 0.5808 & 0.2683 & 0.3391 & 0.8352 & 0.2541 \\\\ \\hline\n",
      "100 & 0.6132 & 0.2735 & 0.3473 & 0.8371 & 0.2578 & 0.5818 & 0.2682 & 0.3391 & 0.8352 & 0.2539 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV1_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.6124 & 0.2196 & 0.2789 & 0.8257 & 0.2454 & 0.5113 & 0.2211 & 0.2790 & 0.8248 & 0.2439 \\\\ \\hline\n",
      "10 & 0.6211 & 0.2790 & 0.3552 & 0.8382 & 0.2579 & 0.5775 & 0.2712 & 0.3426 & 0.8352 & 0.2505 \\\\ \\hline\n",
      "25 & 0.6593 & 0.3403 & 0.4234 & 0.8486 & 0.2762 & 0.6202 & 0.3265 & 0.4049 & 0.8432 & 0.2677 \\\\ \\hline\n",
      "50 & 0.6692 & 0.3534 & 0.4395 & 0.8517 & 0.2834 & 0.6203 & 0.3349 & 0.4151 & 0.8440 & 0.2703 \\\\ \\hline\n",
      "75 & 0.6703 & 0.3566 & 0.4421 & 0.8521 & 0.2843 & 0.6238 & 0.3383 & 0.4184 & 0.8445 & 0.2698 \\\\ \\hline\n",
      "100 & 0.6717 & 0.3566 & 0.4422 & 0.8523 & 0.2845 & 0.6265 & 0.3388 & 0.4191 & 0.8448 & 0.2710 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV2_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.5669 & 0.2172 & 0.2806 & 0.8268 & 0.2469 & 0.5187 & 0.2160 & 0.2772 & 0.8254 & 0.2437 \\\\ \\hline\n",
      "10 & 0.6284 & 0.3017 & 0.3815 & 0.8402 & 0.2601 & 0.5988 & 0.2945 & 0.3711 & 0.8370 & 0.2518 \\\\ \\hline\n",
      "25 & 0.6793 & 0.3799 & 0.4664 & 0.8550 & 0.2889 & 0.6354 & 0.3577 & 0.4383 & 0.8475 & 0.2755 \\\\ \\hline\n",
      "50 & 0.7033 & 0.4092 & 0.5000 & 0.8614 & 0.3036 & 0.6482 & 0.3806 & 0.4636 & 0.8508 & 0.2827 \\\\ \\hline\n",
      "75 & 0.7030 & 0.4191 & 0.5100 & 0.8627 & 0.3070 & 0.6466 & 0.3903 & 0.4740 & 0.8513 & 0.2822 \\\\ \\hline\n",
      "100 & 0.7096 & 0.4210 & 0.5130 & 0.8634 & 0.3084 & 0.6467 & 0.3914 & 0.4749 & 0.8513 & 0.2820 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP10\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.5737 & 0.0184 & 0.0345 & 0.8079 & 0.2309 & 0.4613 & 0.0171 & 0.0320 & 0.8064 & 0.2316 \\\\ \\hline\n",
      "10 & 0.9790 & 0.1264 & 0.2047 & 0.8322 & 0.2573 & 0.6425 & 0.0799 & 0.1305 & 0.8189 & 0.2420 \\\\ \\hline\n",
      "20 & 0.9934 & 0.3869 & 0.5235 & 0.8893 & 0.4036 & 0.7557 & 0.1529 & 0.2265 & 0.8306 & 0.2552 \\\\ \\hline\n",
      "30 & 0.9975 & 0.5755 & 0.7051 & 0.9277 & 0.5664 & 0.7529 & 0.1881 & 0.2676 & 0.8354 & 0.2596 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.6688 & 0.0537 & 0.0886 & 0.8160 & 0.2377 & 0.5070 & 0.0489 & 0.0804 & 0.8136 & 0.2370 \\\\ \\hline\n",
      "10 & 0.9677 & 0.1729 & 0.2648 & 0.8418 & 0.2710 & 0.7117 & 0.1176 & 0.1789 & 0.8265 & 0.2519 \\\\ \\hline\n",
      "20 & 0.9961 & 0.4899 & 0.6279 & 0.9102 & 0.4745 & 0.7806 & 0.1923 & 0.2735 & 0.8379 & 0.2673 \\\\ \\hline\n",
      "30 & 0.9989 & 0.6988 & 0.8086 & 0.9501 & 0.6747 & 0.7573 & 0.2340 & 0.3219 & 0.8432 & 0.2747 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV0_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.4132 & 0.0832 & 0.1198 & 0.8192 & 0.2415 & 0.2925 & 0.0797 & 0.1152 & 0.8172 & 0.2408 \\\\ \\hline\n",
      "10 & 0.9444 & 0.2479 & 0.3415 & 0.8565 & 0.3052 & 0.6158 & 0.1482 & 0.2056 & 0.8247 & 0.2467 \\\\ \\hline\n",
      "20 & 0.9995 & 0.9598 & 0.9790 & 0.9934 & 0.9466 & 0.5281 & 0.1907 & 0.2585 & 0.8233 & 0.2371 \\\\ \\hline\n",
      "30 & 0.9999 & 0.9713 & 0.9853 & 0.9953 & 0.9639 & 0.5311 & 0.1770 & 0.2444 & 0.8233 & 0.2395 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV1_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.4731 & 0.0846 & 0.1210 & 0.8193 & 0.2422 & 0.4800 & 0.0809 & 0.1163 & 0.8174 & 0.2406 \\\\ \\hline\n",
      "10 & 0.9545 & 0.2839 & 0.3892 & 0.8642 & 0.3254 & 0.6179 & 0.1502 & 0.2103 & 0.8250 & 0.2450 \\\\ \\hline\n",
      "20 & 0.9995 & 0.9631 & 0.9807 & 0.9940 & 0.9508 & 0.5471 & 0.1866 & 0.2559 & 0.8237 & 0.2372 \\\\ \\hline\n",
      "30 & 0.9999 & 0.9735 & 0.9865 & 0.9957 & 0.9657 & 0.5684 & 0.1738 & 0.2425 & 0.8239 & 0.2376 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV2_HADM_TOP10\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.4032 & 0.0878 & 0.1258 & 0.8202 & 0.2418 & 0.3673 & 0.0835 & 0.1202 & 0.8182 & 0.2425 \\\\ \\hline\n",
      "10 & 0.9567 & 0.3005 & 0.4094 & 0.8678 & 0.3320 & 0.5656 & 0.1523 & 0.2140 & 0.8254 & 0.2450 \\\\ \\hline\n",
      "20 & 0.9997 & 0.9645 & 0.9815 & 0.9943 & 0.9532 & 0.5296 & 0.1857 & 0.2543 & 0.8228 & 0.2356 \\\\ \\hline\n",
      "30 & 0.9999 & 0.9734 & 0.9864 & 0.9956 & 0.9655 & 0.5465 & 0.1715 & 0.2403 & 0.8234 & 0.2381 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV1_HADM_TOP10\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.9510 & 0.6080 & 0.7337 & 0.9594 & 0.2774 & 0.5207 & 0.2005 & 0.2709 & 0.9211 & 0.0908 \\\\ \\hline\n",
      "10 & 0.9956 & 0.9704 & 0.9827 & 0.9953 & 0.8630 & 0.4372 & 0.2711 & 0.3275 & 0.9161 & 0.0823 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3991 & 0.2725 & 0.3176 & 0.9105 & 0.0728 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3944 & 0.2742 & 0.3176 & 0.9099 & 0.0715 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3944 & 0.2742 & 0.3176 & 0.9099 & 0.0715 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3944 & 0.2742 & 0.3176 & 0.9099 & 0.0715 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.8667 & 0.6447 & 0.7353 & 0.9580 & 0.2087 & 0.5327 & 0.2766 & 0.3529 & 0.9234 & 0.0917 \\\\ \\hline\n",
      "10 & 0.9863 & 0.9768 & 0.9815 & 0.9945 & 0.7892 & 0.4372 & 0.3213 & 0.3662 & 0.9148 & 0.0775 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.9999 & 0.3968 & 0.3166 & 0.3490 & 0.9072 & 0.0643 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3930 & 0.3168 & 0.3478 & 0.9066 & 0.0640 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3930 & 0.3168 & 0.3478 & 0.9066 & 0.0640 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.3930 & 0.3168 & 0.3478 & 0.9066 & 0.0640 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV0_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.3486 & 0.1049 & 0.1212 & 0.9185 & 0.0748 & 0.3715 & 0.1054 & 0.1208 & 0.9182 & 0.0784 \\\\ \\hline\n",
      "10 & 0.4779 & 0.1284 & 0.1591 & 0.9206 & 0.0836 & 0.4160 & 0.1227 & 0.1510 & 0.9196 & 0.0820 \\\\ \\hline\n",
      "25 & 0.4787 & 0.1411 & 0.1776 & 0.9218 & 0.0887 & 0.4540 & 0.1359 & 0.1703 & 0.9208 & 0.0891 \\\\ \\hline\n",
      "50 & 0.4815 & 0.1433 & 0.1805 & 0.9219 & 0.0898 & 0.4630 & 0.1382 & 0.1737 & 0.9210 & 0.0899 \\\\ \\hline\n",
      "75 & 0.5016 & 0.1434 & 0.1806 & 0.9219 & 0.0899 & 0.4589 & 0.1384 & 0.1738 & 0.9209 & 0.0899 \\\\ \\hline\n",
      "100 & 0.4812 & 0.1434 & 0.1806 & 0.9219 & 0.0901 & 0.4580 & 0.1385 & 0.1739 & 0.9209 & 0.0901 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV1_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.3948 & 0.1118 & 0.1326 & 0.9190 & 0.0781 & 0.3712 & 0.1106 & 0.1298 & 0.9185 & 0.0808 \\\\ \\hline\n",
      "10 & 0.5056 & 0.1522 & 0.1932 & 0.9222 & 0.0860 & 0.4552 & 0.1438 & 0.1816 & 0.9208 & 0.0830 \\\\ \\hline\n",
      "25 & 0.5720 & 0.1967 & 0.2541 & 0.9255 & 0.0982 & 0.4872 & 0.1795 & 0.2304 & 0.9229 & 0.0935 \\\\ \\hline\n",
      "50 & 0.5900 & 0.2098 & 0.2721 & 0.9265 & 0.1029 & 0.4866 & 0.1899 & 0.2444 & 0.9229 & 0.0928 \\\\ \\hline\n",
      "75 & 0.5913 & 0.2121 & 0.2743 & 0.9267 & 0.1040 & 0.4852 & 0.1910 & 0.2456 & 0.9229 & 0.0927 \\\\ \\hline\n",
      "100 & 0.5889 & 0.2121 & 0.2745 & 0.9267 & 0.1048 & 0.4862 & 0.1908 & 0.2456 & 0.9230 & 0.0922 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV2_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.3755 & 0.1105 & 0.1321 & 0.9192 & 0.0794 & 0.3843 & 0.1081 & 0.1280 & 0.9185 & 0.0801 \\\\ \\hline\n",
      "10 & 0.5246 & 0.1643 & 0.2092 & 0.9228 & 0.0888 & 0.4581 & 0.1558 & 0.1978 & 0.9213 & 0.0863 \\\\ \\hline\n",
      "25 & 0.5995 & 0.2304 & 0.2978 & 0.9276 & 0.1047 & 0.5059 & 0.2058 & 0.2649 & 0.9239 & 0.0946 \\\\ \\hline\n",
      "50 & 0.6340 & 0.2612 & 0.3379 & 0.9300 & 0.1136 & 0.5112 & 0.2262 & 0.2903 & 0.9244 & 0.0951 \\\\ \\hline\n",
      "75 & 0.6444 & 0.2711 & 0.3512 & 0.9307 & 0.1169 & 0.5067 & 0.2337 & 0.2990 & 0.9243 & 0.0921 \\\\ \\hline\n",
      "100 & 0.6515 & 0.2747 & 0.3554 & 0.9311 & 0.1193 & 0.5033 & 0.2367 & 0.3015 & 0.9241 & 0.0916 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP50\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.1811 & 0.0392 & 0.0476 & 0.9155 & 0.0790 & 0.1350 & 0.0362 & 0.0438 & 0.9146 & 0.0765 \\\\ \\hline\n",
      "10 & 0.7685 & 0.0854 & 0.1034 & 0.9213 & 0.1068 & 0.2690 & 0.0644 & 0.0723 & 0.9175 & 0.0864 \\\\ \\hline\n",
      "20 & 0.9976 & 0.2087 & 0.2874 & 0.9375 & 0.1515 & 0.4207 & 0.0818 & 0.0965 & 0.9201 & 0.0904 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.1999 & 0.0548 & 0.0640 & 0.9174 & 0.0858 & 0.1679 & 0.0512 & 0.0596 & 0.9165 & 0.0817 \\\\ \\hline\n",
      "10 & 0.8084 & 0.1037 & 0.1293 & 0.9242 & 0.1148 & 0.2942 & 0.0745 & 0.0857 & 0.9195 & 0.0885 \\\\ \\hline\n",
      "20 & 0.9985 & 0.2852 & 0.3866 & 0.9451 & 0.1762 & 0.5377 & 0.0953 & 0.1155 & 0.9220 & 0.0928 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV0_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.1586 & 0.0714 & 0.0733 & 0.9180 & 0.0883 & 0.1603 & 0.0684 & 0.0705 & 0.9172 & 0.0854 \\\\ \\hline\n",
      "10 & 0.9809 & 0.1552 & 0.2068 & 0.9291 & 0.1311 & 0.3749 & 0.0843 & 0.0968 & 0.9190 & 0.0861 \\\\ \\hline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: null was reset!\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$reset$1.apply(BrokerState.scala:191)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$reset$1.apply(BrokerState.scala:189)\n",
       "scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n",
       "org.apache.toree.interpreter.broker.BrokerState.reset(BrokerState.scala:189)\n",
       "org.apache.toree.kernel.interpreter.pyspark.PySparkService$$anonfun$pySparkProcess$2.apply(PySparkService.scala:63)\n",
       "org.apache.toree.kernel.interpreter.pyspark.PySparkService$$anonfun$pySparkProcess$2.apply(PySparkService.scala:61)\n",
       "org.apache.toree.interpreter.broker.BrokerProcessHandler.onProcessFailed(BrokerProcessHandler.scala:60)\n",
       "org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:203)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP50\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP50\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP50\", depths=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_WORD2VECV0_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "20 & 0.9994 & 0.9222 & 0.9587 & 0.9944 & 0.8512 & 0.4048 & 0.0938 & 0.1137 & 0.9186 & 0.0833 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV1_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.1775 & 0.0719 & 0.0741 & 0.9181 & 0.0903 & 0.1271 & 0.0671 & 0.0694 & 0.9172 & 0.0843 \\\\ \\hline\n",
      "10 & 0.9841 & 0.1731 & 0.2336 & 0.9313 & 0.1379 & 0.2982 & 0.0849 & 0.0972 & 0.9190 & 0.0870 \\\\ \\hline\n",
      "20 & 0.9992 & 0.9239 & 0.9595 & 0.9946 & 0.8503 & 0.4097 & 0.0922 & 0.1117 & 0.9186 & 0.0838 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV2_HADM_TOP50\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.1625 & 0.0736 & 0.0767 & 0.9184 & 0.0909 & 0.1493 & 0.0688 & 0.0720 & 0.9175 & 0.0869 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP50\", depths=[20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV1_HADM_TOP50\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP50\", depths=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP50\", depths=[10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.9040 & 0.5829 & 0.7063 & 0.8697 & 0.3606 & 0.7299 & 0.4057 & 0.5188 & 0.7958 & 0.1996 \\\\ \\hline\n",
      "10 & 0.9770 & 0.9354 & 0.9557 & 0.9760 & 0.8411 & 0.6444 & 0.5399 & 0.5869 & 0.7922 & 0.1831 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5948 & 0.5505 & 0.5715 & 0.7733 & 0.1568 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5900 & 0.5488 & 0.5682 & 0.7716 & 0.1546 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5900 & 0.5488 & 0.5682 & 0.7716 & 0.1546 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5900 & 0.5488 & 0.5682 & 0.7716 & 0.1546 \\\\ \\hline\n",
      "./data/DATA_TFIDFV1_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.8451 & 0.6144 & 0.7082 & 0.8663 & 0.3091 & 0.7317 & 0.4859 & 0.5807 & 0.8116 & 0.2161 \\\\ \\hline\n",
      "10 & 0.9437 & 0.9309 & 0.9372 & 0.9652 & 0.7276 & 0.6458 & 0.5856 & 0.6141 & 0.7994 & 0.1896 \\\\ \\hline\n",
      "25 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.9998 & 0.5875 & 0.5681 & 0.5776 & 0.7724 & 0.1536 \\\\ \\hline\n",
      "50 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5845 & 0.5672 & 0.5757 & 0.7708 & 0.1503 \\\\ \\hline\n",
      "75 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5845 & 0.5672 & 0.5757 & 0.7708 & 0.1503 \\\\ \\hline\n",
      "100 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.5845 & 0.5672 & 0.5757 & 0.7708 & 0.1503 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV0_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.6232 & 0.3537 & 0.4405 & 0.7687 & 0.1668 & 0.6234 & 0.3532 & 0.4381 & 0.7662 & 0.1680 \\\\ \\hline\n",
      "10 & 0.6629 & 0.4032 & 0.4942 & 0.7845 & 0.1767 & 0.6611 & 0.3989 & 0.4894 & 0.7811 & 0.1752 \\\\ \\hline\n",
      "25 & 0.6761 & 0.4303 & 0.5194 & 0.7917 & 0.1849 & 0.6718 & 0.4251 & 0.5140 & 0.7876 & 0.1814 \\\\ \\hline\n",
      "50 & 0.6787 & 0.4317 & 0.5213 & 0.7927 & 0.1867 & 0.6733 & 0.4250 & 0.5144 & 0.7882 & 0.1842 \\\\ \\hline\n",
      "75 & 0.6789 & 0.4318 & 0.5215 & 0.7928 & 0.1864 & 0.6726 & 0.4251 & 0.5141 & 0.7880 & 0.1837 \\\\ \\hline\n",
      "100 & 0.6784 & 0.4317 & 0.5212 & 0.7926 & 0.1864 & 0.6731 & 0.4254 & 0.5146 & 0.7882 & 0.1837 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV1_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.6413 & 0.3315 & 0.4225 & 0.7699 & 0.1692 & 0.6430 & 0.3295 & 0.4189 & 0.7670 & 0.1713 \\\\ \\hline\n",
      "10 & 0.6815 & 0.4382 & 0.5271 & 0.7939 & 0.1850 & 0.6787 & 0.4313 & 0.5202 & 0.7894 & 0.1821 \\\\ \\hline\n",
      "25 & 0.7135 & 0.4956 & 0.5800 & 0.8114 & 0.2074 & 0.7052 & 0.4814 & 0.5669 & 0.8049 & 0.2033 \\\\ \\hline\n",
      "50 & 0.7224 & 0.5092 & 0.5926 & 0.8161 & 0.2144 & 0.7056 & 0.4914 & 0.5743 & 0.8069 & 0.2046 \\\\ \\hline\n",
      "75 & 0.7244 & 0.5108 & 0.5946 & 0.8168 & 0.2160 & 0.7055 & 0.4913 & 0.5743 & 0.8069 & 0.2056 \\\\ \\hline\n",
      "100 & 0.7243 & 0.5103 & 0.5940 & 0.8168 & 0.2159 & 0.7048 & 0.4899 & 0.5732 & 0.8065 & 0.2044 \\\\ \\hline\n",
      "./data/DATA_WORD2VECV2_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.6448 & 0.3142 & 0.4054 & 0.7693 & 0.1695 & 0.6509 & 0.3134 & 0.4036 & 0.7665 & 0.1722 \\\\ \\hline\n",
      "10 & 0.6870 & 0.4507 & 0.5386 & 0.7970 & 0.1878 & 0.6815 & 0.4423 & 0.5301 & 0.7919 & 0.1843 \\\\ \\hline\n",
      "25 & 0.7295 & 0.5266 & 0.6075 & 0.8212 & 0.2229 & 0.7193 & 0.5084 & 0.5912 & 0.8132 & 0.2130 \\\\ \\hline\n",
      "50 & 0.7477 & 0.5458 & 0.6270 & 0.8294 & 0.2366 & 0.7233 & 0.5222 & 0.6024 & 0.8172 & 0.2201 \\\\ \\hline\n",
      "75 & 0.7503 & 0.5543 & 0.6338 & 0.8316 & 0.2405 & 0.7221 & 0.5281 & 0.6064 & 0.8178 & 0.2217 \\\\ \\hline\n",
      "100 & 0.7511 & 0.5567 & 0.6360 & 0.8322 & 0.2401 & 0.7205 & 0.5305 & 0.6077 & 0.8178 & 0.2211 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP10CAT\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP10CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP10CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP10CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP10CAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/DATA_TFIDFV0_HADM_TOP10CAT\n",
      "Train, Test: 26363 13182\n",
      "iter & train prec & recall & f1 & accuracy & em & test prec & recall & f1 & accuracy & em\n",
      "5 & 0.9397 & 0.0415 & 0.0772 & 0.7383 & 0.1567 & 0.9034 & 0.0371 & 0.0691 & 0.7340 & 0.1610 \\\\ \\hline\n",
      "10 & 0.9624 & 0.2112 & 0.3363 & 0.7830 & 0.1891 & 0.8200 & 0.1442 & 0.2337 & 0.7577 & 0.1710 \\\\ \\hline\n",
      "20 & 0.9909 & 0.5356 & 0.6904 & 0.8735 & 0.4016 & 0.7706 & 0.2590 & 0.3766 & 0.7784 & 0.1859 \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP10CAT\")\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP10CAT\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP10CAT\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV1_HADM_TOP10CAT\")\n",
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP10CAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_experiment(\"./data/DATA_TFIDFV0_HADM_TOP50CAT\")\n",
    "run_experiment(\"./data/DATA_TFIDFV1_HADM_TOP50CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV0_HADM_TOP50CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV1_HADM_TOP50CAT\")\n",
    "run_experiment(\"./data/DATA_WORD2VECV2_HADM_TOP50CAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_experiment2(\"./data/DATA_TFIDFV0_HADM_TOP50CAT\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_TFIDFV1_HADM_TOP50CAT\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV0_HADM_TOP50CAT\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV1_HADM_TOP50CAT\", depths=[5, 10, 20])\n",
    "run_experiment2(\"./data/DATA_WORD2VECV2_HADM_TOP50CAT\", depths=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
