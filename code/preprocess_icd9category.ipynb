{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MIMIC III Preprocessing (per icd9 category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('chartdate', 'date'), ('category', 'string'), ('description', 'string'), ('cgid', 'int'), ('iserror', 'int'), ('text', 'string')]\n",
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('seq_num', 'int'), ('icd9_code', 'string')]\n",
      "[('row_id', 'bigint'), ('subject_id', 'bigint'), ('hadm_id', 'bigint'), ('seq_num', 'bigint'), ('icd9_code', 'string')]\n",
      "[('hadm_id', 'int')]\n",
      "[('subject_id', 'int')]\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()\n",
    "\n",
    "ne_struct = StructType([StructField(\"row_id\", IntegerType(), True),\n",
    "                      StructField(\"subject_id\", IntegerType(), True),\n",
    "                      StructField(\"hadm_id\", IntegerType(), True),\n",
    "                      StructField(\"chartdate\", DateType(), True),\n",
    "                      StructField(\"category\", StringType(), True),\n",
    "                      StructField(\"description\", StringType(), True),\n",
    "                      StructField(\"cgid\", IntegerType(), True),\n",
    "                      StructField(\"iserror\", IntegerType(), True),\n",
    "                      StructField(\"text\", StringType(), True)])\n",
    "df_ne = spark.read.csv(\"./data/NOTEEVENTS-2.csv\",\n",
    "# df_ne = spark.read.csv(\"./data/NOTEEVENTS-2sample.csv\",\n",
    "                       header=True,\n",
    "                       schema=ne_struct)\n",
    "df_ne.registerTempTable(\"noteevents\")\n",
    "df_ne.filter(df_ne.category==\"Discharge summary\") \\\n",
    "    .registerTempTable(\"noteevents2\")\n",
    "    \n",
    "# i want to cache noteevents, but it's too big\n",
    "\n",
    "# many icd to one hadm_id\n",
    "diag_struct = StructType([StructField(\"ROW_ID\", IntegerType(), True),\n",
    "                          StructField(\"SUBJECT_ID\", IntegerType(), True),\n",
    "                          StructField(\"HADM_ID\", IntegerType(), True),\n",
    "                          StructField(\"SEQ_NUM\", IntegerType(), True),\n",
    "                          StructField(\"ICD9_CODE\", StringType(), True)])\n",
    "df_diag_m = spark.read.csv(\"./data/DIAGNOSES_ICD.csv\",\n",
    "                           header=True,\n",
    "                           schema=diag_struct) \\\n",
    "            .selectExpr(\"ROW_ID as row_id\", \n",
    "                        \"SUBJECT_ID as subject_id\",\n",
    "                        \"HADM_ID as hadm_id\",\n",
    "                        \"SEQ_NUM as seq_num\",\n",
    "                        \"ICD9_CODE as icd9_code\")\n",
    "    \n",
    "# added to filter out categories\n",
    "geticd9cat_udf = F.udf(lambda x: str(x)[:3], StringType())\n",
    "df_diag_m = df_diag_m.withColumn(\"icd9_code\", geticd9cat_udf(\"icd9_code\"))\n",
    "df_diag_m.registerTempTable(\"diagnoses_icd_m\")\n",
    "df_diag_m.cache()\n",
    "\n",
    "# one icd to one hadm_id (take the smallest seq number as primary)\n",
    "diag_o_rdd = df_diag_m.rdd.sortBy(lambda x: (x.hadm_id, x.subject_id, x.seq_num)) \\\n",
    "    .groupBy(lambda x: x.hadm_id) \\\n",
    "    .mapValues(list) \\\n",
    "    .reduceByKey(lambda x, y: x if x.seq_num < y.seq_num else y) \\\n",
    "    .map(lambda (hid, d): d[0])\n",
    "df_diag_o = spark.createDataFrame(diag_o_rdd)\n",
    "df_diag_o.registerTempTable(\"diagnoses_icd_o\")\n",
    "df_diag_o.cache()\n",
    "\n",
    "# get hadm_id list in noteevents\n",
    "df_hadm_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT hadm_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_hadm_id_list.registerTempTable(\"hadm_id_list\")\n",
    "df_hadm_id_list.cache()\n",
    "\n",
    "# get subject_id list in noteevents\n",
    "df_subject_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT subject_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_subject_id_list.registerTempTable(\"subject_id_list\")\n",
    "df_subject_id_list.cache()\n",
    "\n",
    "df_diag_o2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_o.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_o JOIN hadm_id_list\n",
    "ON diagnoses_icd_o.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_o2.registerTempTable(\"diagnoses_icd_o2\")\n",
    "df_diag_o2.cache()\n",
    "\n",
    "df_diag_m2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_m.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_m JOIN hadm_id_list\n",
    "ON diagnoses_icd_m.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_m2.registerTempTable(\"diagnoses_icd_m2\")\n",
    "df_diag_m2.cache()\n",
    "\n",
    "print df_ne.dtypes\n",
    "print df_diag_m.dtypes\n",
    "print df_diag_o.dtypes\n",
    "print df_hadm_id_list.dtypes\n",
    "print df_subject_id_list.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+\n",
      "|row_id|subject_id|hadm_id|seq_num|icd9_code|\n",
      "+------+----------+-------+-------+---------+\n",
      "|  1297|       109| 172335|      1|      403|\n",
      "|  1298|       109| 172335|      2|      486|\n",
      "|  1299|       109| 172335|      3|      582|\n",
      "|  1300|       109| 172335|      4|      585|\n",
      "|  1301|       109| 172335|      5|      425|\n",
      "|  1302|       109| 172335|      6|      276|\n",
      "|  1303|       109| 172335|      7|      710|\n",
      "|  1304|       109| 172335|      8|      276|\n",
      "|  1305|       109| 172335|      9|      724|\n",
      "|  1306|       109| 172335|     10|      458|\n",
      "+------+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM diagnoses_icd_m\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preprocessing (all icd9 codes)\n",
    "\n",
    "Returns RDD[(hadm_id, list(icd9_codes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "icd9_score_hadm = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "icd9_score_subj = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "def get_id_to_topicd9(id_type, topX):\n",
    "    if id_type == \"hadm_id\":\n",
    "        icd9_score = icd9_score_hadm\n",
    "    else:\n",
    "        icd9_score = icd9_score_subj\n",
    "        \n",
    "    icd9_topX = set([i.icd9_code for i in icd9_score.takeOrdered(topX, key=lambda x: -x.score)])\n",
    "    \n",
    "    id_to_topicd9 = df_diag_m2.rdd \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.icd9_code)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: set(x) & icd9_topX) \\\n",
    "        .filter(lambda (x, y): y)\n",
    "        \n",
    "    return id_to_topicd9, list(icd9_topX)\n",
    "\n",
    "# for i in get_id_to_topicd9(\"hadm_id\", 10)[0].take(3):\n",
    "#     print i\n",
    "# for i in get_id_to_topicd9(\"subject_id\", 50)[0].take(3):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Obtain dataframe for the merged noteevents and ID-to-ICD9 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sparse2vec(mapper, data):\n",
    "    out = [0] * len(mapper)\n",
    "    if data != None:\n",
    "        for i in data:\n",
    "            out[mapper[i]] = 1\n",
    "    return out\n",
    "\n",
    "def get_id_to_texticd9(id_type, topX):\n",
    "    id_to_topicd9, topicd9 = get_id_to_topicd9(id_type, topX)\n",
    "    mapper = dict(zip(topicd9, range(topX)))\n",
    "    \n",
    "    ne_topX = df_ne.rdd \\\n",
    "        .filter(lambda x: x.category == \"Discharge summary\") \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.text)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: \" \".join(x)) \\\n",
    "        #.join(id_to_topicd9) \\ # involve only data related to top10\n",
    "        # involve all data, even those not related to top10\n",
    "        .leftOuterJoin(id_to_topicd9) \\\n",
    "        .map(lambda (id_, (text, icd9)): \\\n",
    "             [id_, text]+sparse2vec(mapper, icd9))\n",
    "#              list(Vectors.sparse(topX, dict.fromkeys(map(lambda x: mapper[x], icd9), 1))))\n",
    "        \n",
    "    return spark.createDataFrame(ne_topX, [\"id\", \"text\"]+topicd9), mapper\n",
    "\n",
    "# get_id_to_texticd9(\"hadm_id\", 10)[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'584': 0, u'401': 6, u'428': 4, u'414': 3, u'518': 2, u'272': 5, u'276': 1, u'250': 7, u'285': 8, u'427': 9}\n",
      "52726\n"
     ]
    }
   ],
   "source": [
    "df_id2texticd9, topicd9_mapper = get_id_to_texticd9(\"hadm_id\", 10)\n",
    "df_id2texticd9.write.csv(\"./data/DATA_HADM_TOP10CAT\", header=True)\n",
    "\n",
    "print topicd9_mapper\n",
    "print df_id2texticd9.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Test] Load csv file\n",
    "count should be the same with the sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               text  584  276  518  \\\n",
      "0  117760  \"Admission Date:  [**2118-12-14**]            ...    0    0    1   \n",
      "1  129030  Admission Date:  [**2137-8-31**]              ...    0    0    0   \n",
      "2  172040  Admission Date:  [**2174-1-6**]              D...    1    1    0   \n",
      "3  156170  Admission Date:  [**2102-6-9**]              D...    1    0    0   \n",
      "4  199180  Admission Date:  [**2164-7-2**]       Discharg...    0    0    0   \n",
      "\n",
      "   414  428  272  401  250  285  427  \n",
      "0    0    0    0    0    0    0    0  \n",
      "1    0    0    1    1    0    1    0  \n",
      "2    1    0    0    0    0    0    0  \n",
      "3    0    1    0    0    1    1    1  \n",
      "4    1    1    0    0    1    0    0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/DATA_HADM_TOP10CAT.csv\", escapechar='\\\\')\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|icd9_code|\n",
      "+---------+\n",
      "|      401|\n",
      "|      427|\n",
      "|      276|\n",
      "|      272|\n",
      "|      414|\n",
      "|      250|\n",
      "|      428|\n",
      "|      518|\n",
      "|      285|\n",
      "|      584|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n",
    "    \n",
    "# id_to_topicd9, topicd9 = get_id_to_topicd9(\"hadm_id\", 10)\n",
    "# print id_to_topicd9.count()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# SELECT COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "# FROM diagnoses_icd_m2\n",
    "# WHERE icd9_code IN\n",
    "#     (SELECT icd9_code\n",
    "#     FROM diagnoses_icd_m2\n",
    "#     GROUP BY icd9_code\n",
    "#     ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "#     LIMIT 10)\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\"\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
