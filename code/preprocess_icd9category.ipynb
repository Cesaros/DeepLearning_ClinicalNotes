{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC III Preprocessing (per icd9 category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('chartdate', 'date'), ('category', 'string'), ('description', 'string'), ('cgid', 'int'), ('iserror', 'int'), ('text', 'string')]\n",
      "[('row_id', 'int'), ('subject_id', 'int'), ('hadm_id', 'int'), ('seq_num', 'int'), ('icd9_code', 'string')]\n",
      "[('row_id', 'bigint'), ('subject_id', 'bigint'), ('hadm_id', 'bigint'), ('seq_num', 'bigint'), ('icd9_code', 'string')]\n",
      "[('hadm_id', 'int')]\n",
      "[('subject_id', 'int')]\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"preprocess\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"preprocess\").getOrCreate()\n",
    "\n",
    "ne_struct = StructType([StructField(\"row_id\", IntegerType(), True),\n",
    "                      StructField(\"subject_id\", IntegerType(), True),\n",
    "                      StructField(\"hadm_id\", IntegerType(), True),\n",
    "                      StructField(\"chartdate\", DateType(), True),\n",
    "                      StructField(\"category\", StringType(), True),\n",
    "                      StructField(\"description\", StringType(), True),\n",
    "                      StructField(\"cgid\", IntegerType(), True),\n",
    "                      StructField(\"iserror\", IntegerType(), True),\n",
    "                      StructField(\"text\", StringType(), True)])\n",
    "df_ne = spark.read.csv(\"./data/NOTEEVENTS-2.csv\",\n",
    "# df_ne = spark.read.csv(\"./data/NOTEEVENTS-2sample.csv\",\n",
    "                       header=True,\n",
    "                       schema=ne_struct)\n",
    "df_ne.registerTempTable(\"noteevents\")\n",
    "df_ne.filter(df_ne.category==\"Discharge summary\") \\\n",
    "    .registerTempTable(\"noteevents2\")\n",
    "    \n",
    "# i want to cache noteevents, but it's too big\n",
    "\n",
    "# many icd to one hadm_id\n",
    "diag_struct = StructType([StructField(\"ROW_ID\", IntegerType(), True),\n",
    "                          StructField(\"SUBJECT_ID\", IntegerType(), True),\n",
    "                          StructField(\"HADM_ID\", IntegerType(), True),\n",
    "                          StructField(\"SEQ_NUM\", IntegerType(), True),\n",
    "                          StructField(\"ICD9_CODE\", StringType(), True)])\n",
    "df_diag_m = spark.read.csv(\"./data/DIAGNOSES_ICD.csv\",\n",
    "                           header=True,\n",
    "                           schema=diag_struct) \\\n",
    "            .selectExpr(\"ROW_ID as row_id\", \n",
    "                        \"SUBJECT_ID as subject_id\",\n",
    "                        \"HADM_ID as hadm_id\",\n",
    "                        \"SEQ_NUM as seq_num\",\n",
    "                        \"ICD9_CODE as icd9_code\")\n",
    "    \n",
    "# added to filter out categories\n",
    "geticd9cat_udf = F.udf(lambda x: str(x)[:3], StringType())\n",
    "df_diag_m = df_diag_m.withColumn(\"icd9_code\", geticd9cat_udf(\"icd9_code\"))\n",
    "df_diag_m.registerTempTable(\"diagnoses_icd_m\")\n",
    "df_diag_m.cache()\n",
    "\n",
    "# one icd to one hadm_id (take the smallest seq number as primary)\n",
    "diag_o_rdd = df_diag_m.rdd.sortBy(lambda x: (x.hadm_id, x.subject_id, x.seq_num)) \\\n",
    "    .groupBy(lambda x: x.hadm_id) \\\n",
    "    .mapValues(list) \\\n",
    "    .reduceByKey(lambda x, y: x if x.seq_num < y.seq_num else y) \\\n",
    "    .map(lambda (hid, d): d[0])\n",
    "df_diag_o = spark.createDataFrame(diag_o_rdd)\n",
    "df_diag_o.registerTempTable(\"diagnoses_icd_o\")\n",
    "df_diag_o.cache()\n",
    "\n",
    "# get hadm_id list in noteevents\n",
    "df_hadm_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT hadm_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_hadm_id_list.registerTempTable(\"hadm_id_list\")\n",
    "df_hadm_id_list.cache()\n",
    "\n",
    "# get subject_id list in noteevents\n",
    "df_subject_id_list = spark.sql(\"\"\"\n",
    "SELECT DISTINCT subject_id FROM noteevents2\n",
    "\"\"\")\n",
    "df_subject_id_list.registerTempTable(\"subject_id_list\")\n",
    "df_subject_id_list.cache()\n",
    "\n",
    "print df_ne.dtypes\n",
    "print df_diag_m.dtypes\n",
    "print df_diag_o.dtypes\n",
    "print df_hadm_id_list.dtypes\n",
    "print df_subject_id_list.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+\n",
      "|row_id|subject_id|hadm_id|seq_num|icd9_code|\n",
      "+------+----------+-------+-------+---------+\n",
      "|  1297|       109| 172335|      1|      403|\n",
      "|  1298|       109| 172335|      2|      486|\n",
      "|  1299|       109| 172335|      3|      582|\n",
      "|  1300|       109| 172335|      4|      585|\n",
      "|  1301|       109| 172335|      5|      425|\n",
      "|  1302|       109| 172335|      6|      276|\n",
      "|  1303|       109| 172335|      7|      710|\n",
      "|  1304|       109| 172335|      8|      276|\n",
      "|  1305|       109| 172335|      9|      724|\n",
      "|  1306|       109| 172335|     10|      458|\n",
      "+------+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM diagnoses_icd_m\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: bigint, subject_id: bigint, hadm_id: bigint, seq_num: bigint, icd9_code: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diag_o2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_o.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_o JOIN hadm_id_list\n",
    "ON diagnoses_icd_o.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_o2.registerTempTable(\"diagnoses_icd_o2\")\n",
    "df_diag_o2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: int, subject_id: int, hadm_id: int, seq_num: int, icd9_code: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_diag_m2 = spark.sql(\"\"\"\n",
    "SELECT row_id, subject_id, diagnoses_icd_m.hadm_id AS hadm_id,\n",
    "seq_num, icd9_code\n",
    "FROM diagnoses_icd_m JOIN hadm_id_list\n",
    "ON diagnoses_icd_m.hadm_id = hadm_id_list.hadm_id\n",
    "\"\"\")\n",
    "df_diag_m2.registerTempTable(\"diagnoses_icd_m2\")\n",
    "df_diag_m2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|\n",
      "+--------+--------------------------+-----------------------+\n",
      "| 2083180|                     46146|                  58361|\n",
      "+--------+--------------------------+-----------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|\n",
      "+--------+--------------------------+-----------------------+\n",
      "|   59652|                     41127|                  52726|\n",
      "+--------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), COUNT(DISTINCT hadm_id)\n",
    "FROM noteevents\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), COUNT(DISTINCT hadm_id)\n",
    "FROM noteevents2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         category|\n",
      "+-----------------+\n",
      "|              ECG|\n",
      "|     Respiratory |\n",
      "|          Nursing|\n",
      "|          General|\n",
      "|          Consult|\n",
      "|             Echo|\n",
      "|        Nutrition|\n",
      "|       Physician |\n",
      "|         Pharmacy|\n",
      "|   Rehab Services|\n",
      "| Case Management |\n",
      "|        Radiology|\n",
      "|    Nursing/other|\n",
      "|Discharge summary|\n",
      "|      Social Work|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(category)\n",
    "FROM noteevents\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diagnoses_icd: many (icd_code) to one (hadm_id)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT ICD9_CODE)|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|  651047|                     46520|                  58976|                      943|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT lower(ICD9_CODE))|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|  651047|                     46520|                  58976|                             943|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT ICD9_CODE)\n",
    "FROM diagnoses_icd_m\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT LOWER(ICD9_CODE))\n",
    "FROM diagnoses_icd_m\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diagnoses_icd: one (icd_code) to one (hadm_id)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT ICD9_CODE)|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "|   58976|                     46520|                  58976|                      652|\n",
      "+--------+--------------------------+-----------------------+-------------------------+\n",
      "\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|count(1)|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT lower(ICD9_CODE))|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "|   58976|                     46520|                  58976|                             652|\n",
      "+--------+--------------------------+-----------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT ICD9_CODE)\n",
    "FROM diagnoses_icd_o\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*), COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT LOWER(ICD9_CODE))\n",
    "FROM diagnoses_icd_o\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check if I really did get \"seq_num = 1\" for all diagnosis, the code below should return empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+---------+\n",
      "|row_id|subject_id|hadm_id|seq_num|icd9_code|\n",
      "+------+----------+-------+-------+---------+\n",
      "+------+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check code\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM diagnoses_icd_o\n",
    "WHERE seq_num <> 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents and diagnoses_icd (one to one)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------+-------------------------+\n",
      "|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT icd9_code)|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "|                     41127|                  52726|                      641|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT icd9_code)\n",
    "FROM diagnoses_icd_o2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 ICD 9 codes based on \"subject_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|icd9_code|sid_count|\n",
      "+---------+---------+\n",
      "|      414|     3503|\n",
      "|      410|     3137|\n",
      "|      038|     2966|\n",
      "|      V30|     2348|\n",
      "|      424|     1691|\n",
      "|      518|     1324|\n",
      "|      428|     1248|\n",
      "|      996|     1199|\n",
      "|      V31|      981|\n",
      "|      431|      948|\n",
      "|      852|      903|\n",
      "|      427|      900|\n",
      "|      998|      726|\n",
      "|      441|      724|\n",
      "|      434|      690|\n",
      "|      486|      654|\n",
      "|      250|      631|\n",
      "|      584|      611|\n",
      "|      578|      606|\n",
      "|      507|      583|\n",
      "|      198|      513|\n",
      "|      430|      491|\n",
      "|      162|      455|\n",
      "|      571|      427|\n",
      "|      801|      419|\n",
      "|      577|      394|\n",
      "|      562|      377|\n",
      "|      415|      363|\n",
      "|      440|      361|\n",
      "|      433|      342|\n",
      "|      997|      324|\n",
      "|      396|      311|\n",
      "|      197|      308|\n",
      "|      805|      278|\n",
      "|      965|      277|\n",
      "|      482|      277|\n",
      "|      432|      268|\n",
      "|      780|      265|\n",
      "|      519|      260|\n",
      "|      437|      254|\n",
      "|      532|      251|\n",
      "|      820|      244|\n",
      "|      851|      243|\n",
      "|      560|      240|\n",
      "|      V34|      235|\n",
      "|      276|      231|\n",
      "|      345|      229|\n",
      "|      291|      225|\n",
      "|      530|      221|\n",
      "|      491|      221|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS sid_count\n",
    "FROM diagnoses_icd_o2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY sid_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 ICD 9 codes based on \"hadm_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|icd9_code|hadm_count|\n",
      "+---------+----------+\n",
      "|      414|      3540|\n",
      "|      038|      3276|\n",
      "|      410|      3228|\n",
      "|      V30|      2348|\n",
      "|      424|      1707|\n",
      "|      518|      1510|\n",
      "|      428|      1460|\n",
      "|      996|      1373|\n",
      "|      V31|       981|\n",
      "|      431|       966|\n",
      "|      427|       962|\n",
      "|      852|       940|\n",
      "|      250|       884|\n",
      "|      441|       782|\n",
      "|      998|       747|\n",
      "|      486|       703|\n",
      "|      434|       693|\n",
      "|      578|       656|\n",
      "|      507|       643|\n",
      "|      584|       634|\n",
      "|      198|       553|\n",
      "|      430|       495|\n",
      "|      571|       483|\n",
      "|      162|       471|\n",
      "|      577|       434|\n",
      "|      801|       419|\n",
      "|      562|       404|\n",
      "|      440|       389|\n",
      "|      415|       367|\n",
      "|      433|       353|\n",
      "|      997|       332|\n",
      "|      197|       328|\n",
      "|      519|       320|\n",
      "|      396|       314|\n",
      "|      291|       314|\n",
      "|      437|       296|\n",
      "|      482|       293|\n",
      "|      432|       285|\n",
      "|      491|       284|\n",
      "|      805|       281|\n",
      "|      965|       281|\n",
      "|      780|       268|\n",
      "|      532|       257|\n",
      "|      560|       254|\n",
      "|      345|       253|\n",
      "|      851|       245|\n",
      "|      820|       244|\n",
      "|      276|       241|\n",
      "|      V34|       235|\n",
      "|      530|       234|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_o2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY hadm_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noteevents and diagnoses_icd (many to one)\n",
    "Basic Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------+-------------------------+\n",
      "|count(DISTINCT subject_id)|count(DISTINCT hadm_id)|count(DISTINCT icd9_code)|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "|                     41127|                  52726|                      942|\n",
      "+--------------------------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT subject_id), \n",
    "COUNT(DISTINCT hadm_id), COUNT(DISTINCT icd9_code)\n",
    "FROM diagnoses_icd_m2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top ICD 9 codes based on \"subject_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|icd9_code|sid_count|\n",
      "+---------+---------+\n",
      "|      401|    17551|\n",
      "|      427|    13666|\n",
      "|      276|    12326|\n",
      "|      272|    12023|\n",
      "|      414|    11693|\n",
      "|      518|    11063|\n",
      "|      285|    10479|\n",
      "|      250|    10072|\n",
      "|      428|     9974|\n",
      "|      584|     9300|\n",
      "|      V45|     6897|\n",
      "|      599|     6355|\n",
      "|      530|     6010|\n",
      "|      E87|     5799|\n",
      "|      V58|     5723|\n",
      "|      038|     5355|\n",
      "|      V10|     4980|\n",
      "|      410|     4918|\n",
      "|      424|     4871|\n",
      "|      997|     4867|\n",
      "|      995|     4818|\n",
      "|      585|     4808|\n",
      "|      780|     4803|\n",
      "|      785|     4687|\n",
      "|      998|     4595|\n",
      "|      458|     4546|\n",
      "|      403|     4510|\n",
      "|      305|     4457|\n",
      "|      486|     4329|\n",
      "|      041|     3966|\n",
      "|      244|     3941|\n",
      "|      V15|     3925|\n",
      "|      496|     3491|\n",
      "|      287|     3487|\n",
      "|      996|     3486|\n",
      "|      790|     3400|\n",
      "|      507|     3335|\n",
      "|      E93|     3202|\n",
      "|      V12|     3163|\n",
      "|      511|     2972|\n",
      "|      348|     2939|\n",
      "|      765|     2905|\n",
      "|      311|     2859|\n",
      "|      E88|     2828|\n",
      "|      412|     2723|\n",
      "|      493|     2692|\n",
      "|      V29|     2580|\n",
      "|      707|     2526|\n",
      "|      774|     2505|\n",
      "|      300|     2449|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS sid_count\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY sid_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top ICD 9 codes based on \"hadm_id\" count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|icd9_code|hadm_count|\n",
      "+---------+----------+\n",
      "|      401|     20646|\n",
      "|      427|     16774|\n",
      "|      276|     14712|\n",
      "|      272|     14212|\n",
      "|      414|     14081|\n",
      "|      250|     13818|\n",
      "|      428|     13330|\n",
      "|      518|     12997|\n",
      "|      285|     12404|\n",
      "|      584|     11147|\n",
      "|      V45|      8846|\n",
      "|      599|      7199|\n",
      "|      530|      7191|\n",
      "|      V58|      6998|\n",
      "|      585|      6764|\n",
      "|      E87|      6483|\n",
      "|      403|      6297|\n",
      "|      V10|      6204|\n",
      "|      038|      6085|\n",
      "|      995|      5480|\n",
      "|      424|      5404|\n",
      "|      410|      5301|\n",
      "|      780|      5296|\n",
      "|      244|      5101|\n",
      "|      997|      5078|\n",
      "|      785|      5048|\n",
      "|      305|      5000|\n",
      "|      998|      4948|\n",
      "|      458|      4935|\n",
      "|      486|      4732|\n",
      "|      V15|      4420|\n",
      "|      041|      4399|\n",
      "|      496|      4296|\n",
      "|      996|      4251|\n",
      "|      287|      3881|\n",
      "|      V12|      3782|\n",
      "|      790|      3672|\n",
      "|      507|      3608|\n",
      "|      E93|      3473|\n",
      "|      493|      3400|\n",
      "|      311|      3347|\n",
      "|      511|      3218|\n",
      "|      412|      3203|\n",
      "|      707|      3117|\n",
      "|      348|      3112|\n",
      "|      765|      2959|\n",
      "|      E88|      2916|\n",
      "|      571|      2865|\n",
      "|      300|      2855|\n",
      "|      733|      2749|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY hadm_count DESC\n",
    "LIMIT 50\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing (all icd9 codes)\n",
    "\n",
    "Returns RDD[(hadm_id, list(icd9_codes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "icd9_score_hadm = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT hadm_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "icd9_score_subj = spark.sql(\"\"\"\n",
    "SELECT icd9_code, COUNT(DISTINCT subject_id) AS score\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "\"\"\").rdd.cache()\n",
    "\n",
    "def get_id_to_topicd9(id_type, topX):\n",
    "    if id_type == \"hadm_id\":\n",
    "        icd9_score = icd9_score_hadm\n",
    "    else:\n",
    "        icd9_score = icd9_score_subj\n",
    "        \n",
    "    icd9_topX = set([i.icd9_code for i in icd9_score.takeOrdered(topX, key=lambda x: -x.score)])\n",
    "    \n",
    "    id_to_topicd9 = df_diag_m2.rdd \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.icd9_code)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: set(x) & icd9_topX) \\\n",
    "        .filter(lambda (x, y): y)\n",
    "        \n",
    "    return id_to_topicd9, list(icd9_topX)\n",
    "\n",
    "# for i in get_id_to_topicd9(\"hadm_id\", 10)[0].take(3):\n",
    "#     print i\n",
    "# for i in get_id_to_topicd9(\"subject_id\", 50)[0].take(3):\n",
    "#     print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain dataframe for the merged noteevents and ID-to-ICD9 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sparse2vec(mapper, data):\n",
    "    out = [0] * len(mapper)\n",
    "    for i in data:\n",
    "        out[mapper[i]] = 1\n",
    "    return out\n",
    "\n",
    "def get_id_to_texticd9(id_type, topX):\n",
    "    id_to_topicd9, topicd9 = get_id_to_topicd9(id_type, topX)\n",
    "    mapper = dict(zip(topicd9, range(topX)))\n",
    "    \n",
    "    ne_topX = df_ne.rdd \\\n",
    "        .filter(lambda x: x.category == \"Discharge summary\") \\\n",
    "        .map(lambda x: (x.hadm_id if id_type==\"hadm_id\" else x.subject_id, x.text)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda x: \" \".join(x)) \\\n",
    "        .join(id_to_topicd9) \\\n",
    "        .map(lambda (id_, (text, icd9)): \\\n",
    "             [id_, text]+sparse2vec(mapper, icd9))\n",
    "#              list(Vectors.sparse(topX, dict.fromkeys(map(lambda x: mapper[x], icd9), 1))))\n",
    "        \n",
    "    return spark.createDataFrame(ne_topX, [\"id\", \"text\"]+topicd9), mapper\n",
    "\n",
    "# get_id_to_texticd9(\"hadm_id\", 10)[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Input df must be RDD[(label, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer, StopWordsRemover\n",
    "\n",
    "def create_TFIDF(sentenceData, inputCol=\"text\", outputCol=\"features\", minDocFreq=3, numFeatures=20):\n",
    "    tokenizer = RegexTokenizer(pattern=\"[.:\\s]+\", inputCol=inputCol, outputCol=\"z_words\")\n",
    "    wordsData = tokenizer.transform(sentenceData)\n",
    "    \n",
    "    remover = StopWordsRemover(inputCol=\"z_words\", outputCol=\"z_filtered\")\n",
    "    wordsDataFiltered = remover.transform(wordsData)\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"z_filtered\", outputCol=\"z_rawFeatures\", numFeatures=numFeatures)\n",
    "    featurizedData = hashingTF.transform(wordsDataFiltered)\n",
    "    # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "    idf = IDF(inputCol=\"z_rawFeatures\", outputCol=outputCol, minDocFreq=minDocFreq)\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    return rescaledData.drop(\"z_words\", \"z_filtered\", \"z_rawFeatures\", inputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import Vectors\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import DataType, StringType\n",
    "\n",
    "def output_csv(df, path):\n",
    "    udf = UserDefinedFunction(lambda x: Vectors.stringify(x), StringType())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    new_df.write.csv(path, header=True)\n",
    "    \n",
    "def read_csv(path):\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    udf = UserDefinedFunction(lambda x: Vectors.parse(x), VectorUDT())\n",
    "    new_df = df.withColumn('features', udf(df.features))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'584': 0, u'401': 6, u'428': 4, u'414': 3, u'518': 2, u'272': 5, u'276': 1, u'250': 7, u'285': 8, u'427': 9}\n",
      "[('id', 'bigint'), ('584', 'bigint'), ('276', 'bigint'), ('518', 'bigint'), ('414', 'bigint'), ('428', 'bigint'), ('272', 'bigint'), ('401', 'bigint'), ('250', 'bigint'), ('285', 'bigint'), ('427', 'bigint'), ('features', 'vector')]\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "|    id|584|276|518|414|428|272|401|250|285|427|            features|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "|117760|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|(40000,[69,372,69...|\n",
      "|129030|  0|  0|  0|  0|  0|  1|  1|  0|  1|  0|(40000,[13,32,83,...|\n",
      "|172040|  1|  1|  0|  1|  0|  0|  0|  0|  0|  0|(40000,[10,69,152...|\n",
      "|156170|  1|  0|  0|  0|  1|  0|  0|  1|  1|  1|(40000,[3,78,130,...|\n",
      "|199180|  0|  0|  0|  1|  1|  0|  0|  1|  0|  0|(40000,[48,62,80,...|\n",
      "|167440|  0|  0|  1|  1|  0|  1|  0|  1|  0|  0|(40000,[207,264,2...|\n",
      "|178710|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|(40000,[574,794,1...|\n",
      "|162840|  0|  0|  0|  1|  1|  1|  0|  0|  1|  1|(40000,[32,130,15...|\n",
      "|189980|  0|  0|  0|  0|  1|  0|  1|  1|  0|  0|(40000,[75,207,33...|\n",
      "|142370|  0|  0|  0|  1|  1|  0|  0|  0|  0|  1|(40000,[22,30,207...|\n",
      "|153640|  0|  1|  1|  1|  0|  0|  0|  0|  1|  0|(40000,[62,63,85,...|\n",
      "|180780|  1|  1|  1|  0|  0|  0|  0|  1|  1|  0|(40000,[207,648,7...|\n",
      "|149040|  0|  0|  1|  1|  0|  0|  1|  0|  1|  0|(40000,[574,794,1...|\n",
      "|101430|  0|  0|  0|  1|  0|  1|  1|  1|  0|  0|(40000,[33,63,71,...|\n",
      "|132020|  0|  0|  0|  1|  0|  1|  1|  0|  0|  0|(40000,[130,187,2...|\n",
      "|112700|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|(40000,[350,442,5...|\n",
      "|155710|  1|  0|  1|  0|  1|  0|  1|  0|  1|  0|(40000,[63,92,115...|\n",
      "|108100|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|(40000,[32,102,20...|\n",
      "|151110|  1|  1|  1|  0|  1|  0|  0|  0|  0|  1|(40000,[63,73,169...|\n",
      "|140300|  0|  1|  0|  0|  0|  0|  0|  0|  1|  1|(40000,[37,83,106...|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_id2texticd9, topicd9_mapper = get_id_to_texticd9(\"hadm_id\", 10)\n",
    "df_id2featurelabel = create_TFIDF(df_id2texticd9, numFeatures=40000)\n",
    "\n",
    "print topicd9_mapper\n",
    "print df_id2featurelabel.dtypes\n",
    "df_id2featurelabel.show()\n",
    "\n",
    "output_csv(df_id2featurelabel, \"./data/DATA_TFIDF_HADM_TOP10CAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Test] Load csv file\n",
    "count should be the same with the sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44419\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "|    id|584|276|518|414|428|272|401|250|285|427|            features|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "|185344|  0|  0|  0|  1|  0|  0|  0|  0|  0|  1|(40000,[20,32,69,...|\n",
      "|126464|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|(40000,[66,207,26...|\n",
      "|169474|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|(40000,[63,80,207...|\n",
      "|180054|  1|  0|  0|  0|  0|  0|  1|  0|  0|  0|(40000,[32,115,13...|\n",
      "|137734|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|(40000,[48,148,20...|\n",
      "|121864|  0|  0|  0|  0|  1|  0|  1|  0|  0|  1|(40000,[273,379,8...|\n",
      "|115884|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|(40000,[100,361,5...|\n",
      "|105994|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|(40000,[20,32,207...|\n",
      "|110594|  0|  0|  0|  1|  0|  1|  0|  1|  0|  0|(40000,[78,107,14...|\n",
      "|176144|  0|  0|  0|  1|  1|  1|  0|  1|  0|  0|(40000,[62,130,20...|\n",
      "|134744|  0|  0|  0|  1|  1|  1|  0|  1|  0|  1|(40000,[2,207,307...|\n",
      "|101394|  0|  0|  0|  0|  0|  0|  1|  1|  0|  1|(40000,[148,207,7...|\n",
      "|128534|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|(40000,[207,574,5...|\n",
      "|171544|  0|  0|  0|  1|  0|  0|  0|  1|  1|  0|(40000,[20,32,48,...|\n",
      "|198684|  1|  0|  0|  0|  1|  1|  0|  1|  0|  0|(40000,[130,193,2...|\n",
      "|118874|  0|  0|  0|  0|  0|  0|  1|  1|  1|  0|(40000,[63,130,18...|\n",
      "|123934|  0|  0|  0|  1|  0|  1|  0|  1|  1|  1|(40000,[104,207,5...|\n",
      "|166944|  0|  1|  0|  0|  1|  0|  0|  0|  1|  0|(40000,[69,130,14...|\n",
      "|151074|  0|  1|  0|  1|  1|  0|  0|  1|  0|  1|(40000,[63,192,37...|\n",
      "|194084|  0|  0|  0|  0|  1|  0|  1|  0|  0|  0|(40000,[273,794,1...|\n",
      "+------+---+---+---+---+---+---+---+---+---+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdf = read_csv(\"./data/DATA_TFIDF_HADM_TOP10CAT\")\n",
    "print testdf.count()\n",
    "testdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|icd9_code|\n",
      "+---------+\n",
      "|      401|\n",
      "|      427|\n",
      "|      276|\n",
      "|      272|\n",
      "|      414|\n",
      "|      250|\n",
      "|      428|\n",
      "|      518|\n",
      "|      285|\n",
      "|      584|\n",
      "+---------+\n",
      "\n",
      "44419\n",
      "+----------+\n",
      "|hadm_count|\n",
      "+----------+\n",
      "|     44419|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT icd9_code\n",
    "FROM diagnoses_icd_m2\n",
    "GROUP BY icd9_code\n",
    "ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()\n",
    "    \n",
    "id_to_topicd9, topicd9 = get_id_to_topicd9(\"hadm_id\", 10)\n",
    "print id_to_topicd9.count()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT hadm_id) AS hadm_count\n",
    "FROM diagnoses_icd_m2\n",
    "WHERE icd9_code IN\n",
    "    (SELECT icd9_code\n",
    "    FROM diagnoses_icd_m2\n",
    "    GROUP BY icd9_code\n",
    "    ORDER BY COUNT(DISTINCT hadm_id) DESC\n",
    "    LIMIT 10)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
